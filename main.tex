% interactapasample.tex
% v1.05 - August 2017

\documentclass[]{interact}

\usepackage{epstopdf}% To incorporate .eps illustrations using PDFLaTeX, etc.
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}
% \usepackage[caption=false]{subfig}% Support for small, `sub' figures and tables
%\usepackage[nolists,tablesfirst]{endfloat}% To `separate' figures and tables from text if required
%\usepackage[doublespacing]{setspace}% To produce a `double spaced' document if required
%\setlength\parindent{24pt}% To increase paragraph indentation when line spacing is doubled

\usepackage{multirow}
\usepackage{tabularx}

\usepackage{hyperref}

% \usepackage[longnamesfirst,sort]{natbib}% Citation support using natbib.sty
% \bibpunct[, ]{(}{)}{;}{a}{,}{,}% Citation support using natbib.sty
% \renewcommand\bibfont{\fontsize{10}{12}\selectfont}% To set the list of references in 10 point font using natbib.sty

% \usepackage[natbibapa,nodoi]{apacite}% Citation support using apacite.sty. Commands using natbib.sty MUST be deactivated first!
% \setlength\bibhang{12pt}% To set the indentation in the list of references using apacite.sty. Commands using natbib.sty MUST be deactivated first!
% \renewcommand\bibliographytypesize{\fontsize{10}{12}\selectfont}% To set the list of references in 10 point font using apacite.sty. Commands using natbib.sty MUST be deactivated first!

\usepackage{apacite}

\theoremstyle{plain}% Theorem-like structures provided by amsthm.sty
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{notation}{Notation}

\begin{document}

\articletype{ARTICLE TEMPLATE}% Specify the article type or omit as appropriate

\title{Sketching Sounds Journal Paper Draft}

\author{
\name{A.~N. Author\textsuperscript{a}\thanks{CONTACT A.~N. Author. Email: latex.helpdesk@tandf.co.uk} and John Smith\textsuperscript{b}}
\affil{\textsuperscript{a}Taylor \& Francis, 4 Park Square, Milton Park, Abingdon, UK; \textsuperscript{b}Institut f\"{u}r Informatik, Albert-Ludwigs-Universit\"{a}t, Freiburg, Germany}
}

\maketitle

\begin{abstract}
\end{abstract}

\begin{keywords}
Sections; lists; figures; tables; mathematics; fonts; references; appendices
\end{keywords}


\section{Introduction}\label{sec:introduction}

Humans make sense of sound in various ways often borrowing from associations with other sensory domains. While arguably most known for connections between sound and colour, these cross-modal associations can take many forms. This research focuses on connections between sound and shapes in graphical sketches. Historically, sound-shape associations have been researched through matching studies where participants are presented with a sound stimulus and multiple visual stimuli. More recent research has participants respond freely through actions like gesture, movement or drawing. Results suggest that to some extent people share similar associations despite individual variance. While existing research mainly looks at associations between shapes and phonetic sounds, pitch or music, this research focuses on the character of a sound or its timbre. This is investigated through two studies that ask participants to sketch their associations with sounds using a digital interface. The first study (n$=$28) uses an exploratory design that imposes minimal restrictions to gain an overview of the various representational approaches participants will deploy for different types of sound. The second study (n$=$88) focuses on simple, abstract representations produced by a frequency modulation (FM) synthesizer. Alongside sound-sketches, qualitative feedback was collected from participants through interviews and surveys to find out how they approached the tasks. The analysis categorised the various representational approaches and tested for correlations between sounds and shapes through statistical analyses of quantitative audio and visual features. The results lay the groundwork for the wider context of this research that aims to find out if a graphical sketch input can be used to help improve interaction with digital music production software, specifically for the control of digital synthesizers. Contemporary music production heavily uses digital technologies and musical timbre occupies a prominent role for many modern music styles that distinguish themselves through their `specific' sound rather than harmony, melody or musical structure. Finding or crafting a desired sound becomes an integral part of the production process which often involves wading through large sample libraries or tweaking software parameters. These parameters often relate to the underlying digital signal processing (DSP) rather than human perception of sound which can make it difficult to realise sound ideas or explore sonic spaces in an intuitive way. With the development of a sketch-controlled synthesizer in mind, the sound-sketches collected in these studies aid the future implementation of a mapping architecture between a sketch input and an FM synthesizer. The process of designing the second study included the development of the digital sketching interface from a simple, generic sketchpad to a specialised sketching interface that could be used for controlling a sketch-based synthesizer.

This paper first introduces relevant research into cross-modal associations, with a focus on sound-shape associations, musical timbre and sketch recognition in Section~\ref{sec:background}. The methodology is described in Section~\ref{sec:methodology}, followed by the setup of both studies in Section~\ref{sec:study_setup}. The results are presented in multiple sections: an overview of extracted audio and visual features in Section~\ref{sec:extracted_features}, analysis of participant feedback in Section~\ref{sec:interview_analysis}, sound-sketch categorisation in Section~\ref{sec:sketch_categorisation} and statistical feature analysis in Section~\ref{sec:feature_analysis}. Discussion and conclusion can be found in Sections~\ref{sec:discussion} and~\ref{sec:conclusion}. 


\section{Background}\label{sec:background}
This section first gives an overview of cross-modal research with a focus on sound-shape associations that build the basis for this research. It then continues to introduce research into timbre and sketch recognition that is relevant for the analysis of the user studies.     

\subsection{Cross-modal associations}\label{subsec:sound-shape}
Cross-modal associations describe how a stimulus from one modality can induce a response in another modality. While often reported between sounds and colour, they can occur across various modalities, for example between colours and odors~\cite{}, sounds and tastes~\cite{} or sound and shapes~\cite{}. Cross-modal associations are sometimes wrongly referred to as synesthesia. Synesthesia is a rare condition occurring in less than 5{\%} of the population~\cite{} and synesthetes experience cross-modal connections involuntarily and consistently; the same stimulus always induces the same response. Cross-modal associations on the other hand are experienced in some form by most people, but connections tend to be far less consistent and might only occur situationally. Cross-modal associations can be found when describing a sound in everyday communication referencing concepts like brightness, warmth, sharpness or colour among others. This requires some level of shared understanding which is supported by research that found consistent cross-modal associations between people despite influence by personal factors~\cite{}. One of the earliest examples of cross-modal research is provided by Wolfgang K{\"o}holer, a member of the \textit{Gestaltpsychology} movement in the 1920s, who found that people associate the made-up words \textit{takete} or \textit{kiki} with sharp, jagged shapes and \textit{maluma} or \textit{boubou} with soft, round shapes~\cite{kohler1929gestalt}. 

\begin{figure}[h]
\centering
\includegraphics[width=0.5\columnwidth]{Images/maluma-takete.jpg}
\caption{Visual stimuli used in Wolfgang K{\"o}hler's experiments. The left shape is overwhelmingly associated with the made-up word `maluma' and the right one with `takete'.}
\end{figure}

The effect was confirmed in multiple studies~\cite{ramachandranSynaesthesiaWindowPerception} and generalised to all phonemes~\cite{nielsenParsingRoleConsonants2013}. It was observed across cultures~\cite{davisFitnessNamesDrawings1961,taylorPhoneticSymbolismFour1962,bremnerBoubaKikiNamibia2013}, age groups including toddlers~\cite{maurerShapeBoubasSound2006}, to some extent, with the visually impaired~\cite{bottiniSoundSymbolismSighted2019} and between movement and phonemes~\cite{shinoharaTaketeMalumaAction2016}. Similar associations were not only found for phonetic sounds but also for musical instruments~\cite{adeliAudiovisualCorrespondenceMusical2014} and abstract sonic textures~\cite{grillVisualizationPerceptualQualities2012}. In these studies, associations are found by explicitly asking participants to match stimuli from different modalities for example words to shapes. 

Another approach to studying cross-modal associations is to measure how a stimulus in one domain can influence the perception of another stimulus in a different domain. A famous example is the McGurk effect~\cite{}, that shows how a video of a speaker voicing different vowels can influence which vowels participants hear in an audio recording. Other examples include the impact of a mug's color on the taste of coffee~\cite{van2014does} and the effect of musical groove on sexual attractiveness~\cite{}. Actions can be also be influenced by sound; Thoret et al. found that participants drew circles with a more elliptical skew when listening to sounds that evoked elliptical kinematics~\cite{thoretSeeingCirclesDrawing2016}. Salgadoâ€‘Montejo et al. showed the influence of pitch on the location of free hand movement. They also found that movement was more jagged for higher pitches and rounder for lower pitches~\cite{spence_paper}. Both examples show that consistencies can be found between people not just in matching or rating tasks but also when given free agency over their response. However, participants' actions were not visualised posing the question if seeing one's action drawn out as a sketch has an influence on the response. 

In visual art, connections between the auditory and visual domains are a reoccurring theme. Notably, Russian artist Wassily Kandinsky developed multiple hypotheses on associations between shapes, colours and music leading to a number of cross-modal artworks inspired by pieces of composer Arnold Sch{\"o}nberg~\cite{}. Consistent associations were found between complex stimuli of visual artworks and musical compositions~\cite{painting&spanish_music_paper} and piano music excerpts and simple visual structures~\cite{clemente2020set}. K{\"u}ssner investigated how participants represent pure tones varied in pitch, loudness and tempo producing similar findings about space and pitch as Salgado-Montejo et al.~\cite{}. 

While timbre is acknowledged to play a role in cross-modal associations, the aforementioned research focuses on the musical context of the sound stimuli. Grill et al. showed that visualisation of cross-modal associations can help participants retrieve sound samples outside of a specific musical context~\cite{grillVisualizationPerceptualQualities2012}. Knees and Andersen further developed this idea by proposing sound retrieval from a graphical sketch input and built a non-functioning prototype of such a system~\cite{kneesSearchingAudioSketching2016}. Compared to K{\"u}ssner's research mentioned above, timbre plays a more dominant role here which participants tend to visualise through shapes, contours or textures - what the researchers call \textit{symbolic}. Recent research further investigated how participants represent different timbres in a more controlled way~\cite{engelnCoHEARenceAudibleShapes2020} and show how a functioning sketch-based sound retrieval pipeline could be implemented with the help of deep learning~\cite{engeln2021similarity}. The results suggest, but also images that could be classified as more complex artwork as discussed above. Work by the authors produced similar results~\cite{icmc2021} and showed that participants can deduct information about sound from sketches~\cite{icmc2022}. The research presented in this paper takes a closer look at how to classify different representational approaches and collects a comprehensive sound-sketch dataset.  

% -  However, an important distinction has to be made to sound-shape associations that relates to the characteristics of sound (musical timbre) and not musical structure like melody and harmony 
% - Sound-shape associations can inform visualisations that communicate sound characteristics without audio playback for example to improve the organisation of one's personal music library~\cite{kolhoffMusicIconsProcedural2006}, sample selection for live DJ performances~\cite{chen2010thumbnaildj}, exploration of different natural sounds~\cite{wan2019towards,ishibashi2020investigating} and retrieval of abstract sounds  

\subsection{Description of musical timbre}\label{subsec:musical_timbre}
The previous section, illustrates how cross-modal associations can be a helpful conversational tool when communicating characteristics of sound that cannot easily be quantified like loudness or pitch. These qualitative aspects of sound are generally described as musical timbre; however, it remains a concept that lacks a unified definition. An often cited definition from the American National Standards Institute (ANSI) states that timbre is the auditory attribute that enables listeners to distinguish two sounds with the same loudness and pitch~\cite{}. This definition 
is often criticised for only describing what timbre is not, but not defining what it actually is~\cite{bregman_or_siedenburg}. While humans typically have an intuitive understanding of what constitutes timbre, the language they use to describe it varies individually~\cite{saitisTimbreSemanticsLens2018}. The perceptual attributes used to process a sound can be influenced by the source itself or individual factors of the listeners like preference, domain knowledge, cultural upbringing or listening context. This becomes apparent in the descriptors of sound libraries and synthesizer presets in digital audio production that can range from describing an acoustic instrument (keys, strings, drums), a compositional function (lead, pad, bass), a playing style (staccato, legato, arpeggio), references to genre (rock, hip-hop), hardware (808, clavinet), jargon (wobble bass, acid), mood (happy, sad) or describing a sound's timbre directly often with the help of cross-modal associations (bright, rough, shrill). Without tagging guidelines or a unified approach to describing sound, retrieving specific sounds from sample or preset libraries can be a cumbersome task that impairs the creative process. An increasingly popular approach for the organisation of these libraries is to display samples in a 2-dimensional space where similar samples are displayed close to each other~\cite{friedAudioQuilt2DArrangements, brufordGrooveExplorerIntelligent2019, garberAudioStellarOpenSource, XO}. The groundwork for this approach was laid by ~\citeA{krumhansl1992perceptual,iversonIsolatingDynamicAttributes1993}, one the first researchers to introduce timbre spaces for acoustic instruments by obtaining dissimilarity ratings through participant studies and using multi-dimensional scaling (MDS) to create a 2-dimensional timbre space. For applications that are designed to work with a user's own sample library like \textit{XO} by XLN Audio or the \textit{Timbral Explorer}\footnote{\url{https://www.audiocommons.org/2019/01/30/timbral-explorer.html}} it is not feasible to use samples annotated by humans, instead they use computationally extracted audio features. A popular and successful feature is the mel-frequency cepstral coefficient (MFCC) that can be used as input for deep-learning classification of diverse environmental sounds~\cite{openl3}. However, MFCCs are not very good at communicating sound characteristics to humans or help explain human perception of sound~\cite{siedenburgComparisonApproachesTimbre2016}. Other popular computational features that, while still based in acoustic feature analysis, show better alignment with human perception are \textit{spectral flatness} and \textit{zero-crossing rate} that describe a sound's noisiness, \textit{spectral centroid} that serves as a measure for brightness and the \textit{root-mean-square} (RMS) describing a sounds loudness~\cite{peeters2004large,cite_libros_website?}. Pearce et al. aim to bridge the disconnect between computational features and features relevant to human perception by developing a model that, through a mixture of human annotated sound samples and computed features, predicts a sound's \textit{hardness, depth, brightness, roughness, warmth, sharpness, booming} and \textit{reverberation}~\cite{pearceModellingTimbralHardness2019} making it possible to quickly computationally obtain perceptually relevant features.   
Hayes et al. pointed out that there is gap in timbre research that focuses on synthesised sounds that do not attempt to mimic acoustic instruments~\cite{hayes_new_journal_paper}. They implemented a simple FM synthesizer to run in a web browser and collected a dataset of annotated synthesizer sounds through an online participant study. Participants were presented with a prompt to change a reference sound and then asked to annotate the result. The prompts were presented in the form \textit{make the sound less/more: bright/rough/thick}. The attributes were derived from the \textit{luminance-texture-mass} LTM model~\cite{zacharakis} that suggests that timbre descriptions can sufficiently be explained by these three dimensions for sounds with similar amplitude envelopes.

\subsection{Sketch recognition}\label{subsec:sketch_recognition}
To statistically evaluate cross-modal associations between sound and shapes both domains have to be described as quantitative features. The extraction of information from hand-drawn sketches by a computer is called sketch recognition. Typical applications include recognition of handwriting or image retrieval from a sketch input. Sketches can be analysed as rasterized images using a computer vision approach. A benchmark task is the classification of handwritten digits with the MNIST dataset that is typicially achieved through a machine learning approach with convolutional neural networks (CNNs) producing the best results~\cite{lecun1998mnist, deng2012mnist}. Representing sketches sequentially either through vectorization or collecting sketches digitally gives the opportunity for a wider range of analyses. The most notable example is seminal work by Ha and Eck~\cite{haNeuralRepresentationSketch2017} that introduced the \textit{SketchRNN} architecture that uses a variational autoencoder (VAE) built on recurrent neural networks (RNNs). \textit{SketchRNN} was trained on \textit{Quick, Draw!}\footnote{\url{https://quickdraw.withgoogle.com/}}, a large-scale, open-source dataset with over 50 million sketches divided into categories ranging from simple geometric shapes to complex representations of animals and objects. \textit{Quick, Draw!} inspired a large number of projects by enabling researchers to experiment with pre-train models and (re-)train them for specific tasks. 

While \textit{SketchRNN} produces impressive results for sketch classification and generation, algorithmic approaches might be more suitable for describing the shape of a sketch. Wolin et al.'s \textit{shortstraw} algorithm~\cite{wolinShortStrawSimpleEffective} provides a simple, effective tool to extract corner points. Xiong et al.~\cite{xiongRevisitingShortStrawImproving2009} extended the algorithm to also recognize curve points. Szegin et al.~\cite{sezginFeaturePointDetection} further show that information can not only be extracted from a sketch's shape but also from the drawing speed, for example corner points can be estimated from slowing down before each point. In the context of sound-shape associations, sketch or movement responses are often interpreted qualitatively by humans rather than computationally as it can be seen in the works by \citeA{salgado_spence,kneesSearchingAudioSketching2016} discussed in Section~\ref{subsec:sound-shape}. \citeA{kussner_paper_about_extracting_features} propose a computational approaches to extracting features from sound-sketches that, however, expect a representation inside a grid where the x-axis represents time. \citeA{engeln2021similarity} harness advancements in deep learning for sketch recognition to implement a computational method for free-form sketch-based sound retrieval. However, an end-to-end retrieval approach might not reveal which aspects of a sketch is relevant sound-shape associations.       


\section{Methodology}\label{sec:methodology}

Sound-sketch datasets were collected in two participant studies, referred to as Study One and Study Two, to investigate how sound-shape associations inform graphical sketch representations of sounds. Participants were asked to sketch their associations with sound stimuli using a digital interface. The resulting sketches were categorised to capture broad stylistic approaches. Quantitative features were extracted from the sound stimuli and sound-sketches to test for statistical correlations. In the broader context, this research investigates if graphical sketches could be used as an input to control sound synthesis. Alongside the perceptual studies, a digital sketching interface was developed through an iterative design process to facilitate experimentation and data collection.

\subsection{Design of perceptual studies}\label{sec:study_design}

As discussed in Section~\ref{subsec:sound-shape}, only little research has been conducted on how humans represent sounds through graphical sketching. The two studies presented in this paper collect sound-sketches by combining existing research that ask participant to match auditory and visual stimuli or undertake specific hand movements while listening to sound with methods of digital sketch collection introduced in Section~\ref{subsec:sketch_recognition}. In both studies, participants are presented with a number of different sound stimuli and are asked to sketch their personal association with them. The first study follows an exploratory design with minimal instructions for participants and different categories of sound from abstract synthesizer pads to acoustic instruments to obtain a wide overview of different representational approaches. Informed by the results of Study One, the second study followed a more controlled design with a further developed interface and a narrower set of synthesised sound stimuli that encouraged simpler, more abstract sketch representations with a stronger link to sound-shape associations. Besides collecting sound-sketches, it is important for this research to understand the reasoning behind different approaches. Qualitative feedback can give a more detailed insight into the thought processes of a participant and might yield information that cannot be captured through quantitative data. For Study One, qualitative data was collected through behavioral observation and a semi-structured interview that probed participants to reflect on their approaches and the interaction with the digital interface. Study Two gave the option to provide brief written feedback and asked participants to answer a modified System Usability Scale (SUS) questionnaire~\cite{lewisSystemUsabilityScale2018}. All verbal feedback was analysed through thematic analysis~\cite{braunUsingThematicAnalysis2006,stuckeySecondStepData2015}. General demographic data including age, gender, occupation and country of origin was collected through survey questions. In addition, the section of the Goldsmiths Music Sophistication Index (Gold MSI)~\cite{mullensiefenGoldsmithsMusicalSophistication} relating to musical training and engagement with music was used to categorise participants by music proficiency.   

\subsection{Sketch categorisation}\label{subsec:qualitative_analysis}
The first step of the analysis categorises different representational approaches that participants deploy and investigates to what extent they are informed by sound-shape associations. For Study One, an open card-sorting study was conducted where six participants (4 female, 3 musicians) who did not take part in the main study were asked to sort the collected sketches into three to ten categories and provide short descriptions for them. The study was completed remotely within three hours on participants' devices following detailed instructions that can be accessed online \footnote{\url{https://youtu.be/LXTlnaAciWw}}. Following the methodology of \citeA{paea2018information}, the results were encoded and reduced in dimensionality using principal component analysis (PCA). K-means clustering was used together with the silhouette coefficient~\cite{rousseeuw1987silhouettes}, a measure of cluster goodness, to find the most suitable number of clusters between three and ten. Clusters were named and described qualitatively based on keywords that participants used in their category descriptions. 

As Study Two produced a considerably larger number of sound-sketches, the categorisation process was automated with the help of machine learning. A variational autoencoder (VAE) using the \textit{SketchRNN} architecture was pre-trained on the \textit{Quick, Draw!} categories \textit{Triangle}, \textit{Square}, \textit{Circle}, \textit{Line}, \textit{Squiggle} and \textit{Zigzag} before feeding in sound-sketches from Study Two. The resulting 128-dimensional latent representation of the dataset was reduced with PCA and the best number of clusters was determined with K-means and the silhouette coefficient as described above. To compare differences in category distribution, sounds were grouped according to their annotated attributes \textit{bright}, \textit{rough} and \textit{thick} derived from the LMT model described in Section~\ref{subsec:musical_timbre}. For each attribute the three sounds with the highest and lowest values were used to form a group. Pearson's Chi-squared test and Cochran's Q test were used to assess whether the sound/sound group or a participant's musical proficiency influenced the representational approach.

\subsection{Quantitative analysis of sketch and sound features}\label{subsec:feature_analysis}

As described in greater detail in Section~\ref{sec:iterative_interface_design}, sketches are saved digitally as sequential data in the following form: 

\begin{lstlisting}[caption=Structure of sketch data]
#Sketch array
    [#Stroke 1 array
        [#Recorded stroke points
            [x0,x1,x2,...], #x positions
            [y0,y1,y2,...], #y positions
            [t0,t1,t2,...], #timestamps
        ],
        #More Stroke arrays
        [...],
        [...],
        ...
    ]
\end{lstlisting}

To describe sketches quantitatively, a number of features can be calculated directly from the data structure and through simple arithmetic operations as demonstrated in Equations 1, 2 and 3, where \textit{$N$} is the number of strokes in a sketch and \textit{$\overline{L}$}, \textit{$\overline{T}$} and $\overline{S}$ are their average length, completion time and drawing speed. The total number of points in the \textit{$k^{th}$} stroke is described by \textit{$n_k$}. Each point has a position \textit{$x_{k_i}$} and timestamp  \textit{$t_{k_i}$}. The euclidean distance between two points is described by $d(p, q)$. 
\begin{align}
    \overline{L} & = \frac{1}{N} \sum_{k=1}^{N} \sum_{i=2}^{n_k} d\left(x_{k_{i}}, x_{k_{i-1}} \right)    
    \\
    \overline{T} & = \frac{1}{N} \sum_{k=1}^{N} t_{k_{n_k}} - t_{k_1}
    \\
    \overline{S} & = \frac{1}{N} \sum_{k=1}^{N} \sum_{i=2}^{n_k} d\left(x_{k_{i}}, x_{k_{i-1}} \right) \frac{1}{t_{k_{n_k}} - t_{k_1}}
\end{align}

Sound-shape associations are usually reported with respect to a shape's contour focusing on their ``jaggedness" or ``roundness"~\cite{adeliAudiovisualCorrespondenceMusical2014,grillVisualizationPerceptualQualities2012}. These attributes were quantified by extracting corner points divided into obtuse, right and acute angles and curve points divided into wide and narrow shape algorithm~\cite{wolinShortStrawSimpleEffective,xiongRevisitingShortStrawImproving2009}. A qualitative review suggested that sketches differ by the number of stroke intersections that can be interpreted as the ``noisiness" of a sketch. The number of intersections was determined using an adaptation of Bresenham's rasterisation algorithm~\cite{bresenham1965algorithm}. Prior to extracting features, the sketch data was cleaned by removing consecutive points with the same position and merging two strokes if a starting point was within a five pixel distance to an end point. The number of intersections, corner and curve points is reported relative to the total stroke length of a sketch.

\begin{figure}[h]
    \centering
     \subfloat[Intersection detection using an adaptation of Bresenham's rasterisation algorithm~\cite{bresenham1965algorithm}. The size of the red dots corresponds to the number of intersections at that point.\label{fig:inter}]{\includegraphics[width=0.3\textwidth]{Images/intersections.png}}
     \hfill
     \subfloat[Feature point detection with the shortstraw algorithm~\cite{wolinShortStrawSimpleEffective,xiongRevisitingShortStrawImproving2009}. Stroke start/end points are coloured green/blue and corner/curve points are coloured red/yellow.\label{fig:shortstraw}]{\includegraphics[width=0.3\textwidth]{Images/shortstrawdetection.png}}
    \hfill
     \subfloat[Different curve shapes according to Xiong and LaViola~\cite{xiongRevisitingShortStrawImproving2009}. For this study, a narrow curve was defined with $\alpha<90^{\circ}$ and a wide curve with $\alpha\geq90^{\circ}$.\label{fig:curves}]{\includegraphics[width=0.3\textwidth]{Images/Curves.png}}
    \caption{Sketch feature extraction}
    \label{fig:sketchfeatures}
\end{figure}


In order to investigate sound-shape associations through statistical analysis, the sound stimuli also have to be described using quantitative features. This was accomplished by computing the mean values of \textit{Centroid Frequency}, \textit{Spectral Flatness}, \textit{Zero Crossing} and \textit{Root Mean Square Power (RMS)} for each sound using the \textit{Librosa} library~\cite{librosa} with a FFT window size of 2048 and hop length of 512. In addition, the \textit{timbral models} by \citeA{pearceModellingTimbralHardness2019} provided quantified measures of \textit{Hardness}, \textit{Depth}, \textit{Brightness}, \textit{Roughness}, \textit{Warmth}, \textit{Sharpness} and \textit{Boominess} which can more easily be related to human perception of sound. The additional feature \textit{RMS Slope}, describing how continuous or intersected a sound is, was quantified by the slope between prominent extrema in the RMS envelope.

The sketch categories described in Section~\ref{subsec:sketch_categorisation} provide a qualitative description of representational approaches. The quantitative set of features described in this section are used to investigate sound-shape associations statistically. Spearman's rank correlation coefficient is used to find out which, if any, visual features correlate with which audio features. As a linear perceptual relationship between features cannot be assumed and the underlying distribution is unknown, the non-parametric Spearman test was chosen as it can find not only linear, but monotonic relationships in general. The correlation analysis uses mean values of visual features for each sound. To ensure that averaged features still pose meaningful sketch descriptions the inter-rater reliability was determined using the ICC(2,k) model intraclass correlation coefficient (ICC)~\cite{koo2016guideline}. The ICC(2,k) measures absolute agreement of average raters by averaging responses of k raters for each subject. In this context, sound stimuli were defined as subjects and sketch features as measurements. Sketch features were first log-transformed to meet the normal distribution assumption of the ICC. This approach was chosen because perceptual data typically includes large variance/noise and patterns emerge more clearly when observing averages. This means that results will provide information about the average sound-sketch representation derived from multiple participants, but might not be applicable for predicting or describing sketches of an individual.   

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\textwidth]{Images/RMS_Slope.png}
%     \caption[width=0.2\textwidth]{Visualisation of the RMS Slope. The light-grey line shows the original RMS envelope, the blue line shows the signal after low-pass filtering and the red line shows the gradients between extrema.}
%     \label{fig:RMS_Slope}
% \end{figure}



\section{Study Setup}\label{sec:study_setup}

The studies, referred to as Study One and Study Two, collect sound-sketches from participants by asking them to sketch their associations with different sound stimuli using a digital interface. The key differences can be summarised as:

\begin{itemize}
    \item \textbf{Study One}: The goal of this study is to gain an overview on how humans represent sound through sketches. Twenty-eight participants were presented with an exploratory study design that includes a range of different sound types while giving little instructions on how to represent them graphically. 
    \item \textbf{Study Two}: a controlled study conducted online with 88 participants. It exclusively uses FM synthesiser sounds and a specialised interface that encourages simple, abstract representations. The goal of this study is to confirm the findings of Study One and collect a sound-sketch dataset to inform the development of a sketch-controlled sound synthesiser.   
\end{itemize}

\subsection{Material}\label{subsec:material}
Both studies are set entirely in a digital environment and sound stimuli are selected with regard to timbral differences. Participants for Study One completed the study on the same laptop and, to keep similar conditions, only laptop or desktop devices were allowed for Study Two.

\subsubsection{Study One}\label{subsubsec:material_one}
The study was conducted in person using the touchpad on a 15" MacBook Pro and a pair of Beyerdynamic DT 770 headphones in calm, indoor locations. A total of ten timbrally dissimilar sounds were selected following the research of Adeli et al.~\cite{adeliAudiovisualCorrespondenceMusical2014} and Grill et al.~\cite{grillVisualizationPerceptualQualities2012} ranging from musical instruments (\textit{Piano}, \textit{Strings}, \textit{Electric Guitar}) and environmental sounds (\textit{Impact}) to synthesised pads (\textit{Telephonic}, \textit{Subbass}) and abstract textures (\textit{Noise}, \textit{String Grains}, \textit{Crackles}, \textit{Processed Guitar}). All sound stimuli are monophonic, normalised for equal loudness, pitched to the MIDI note C3 and last eight seconds including trailing silence to mark a clear endpoint during looped playback. The perceived base frequency may vary due to prominent harmonics. The wide range of sounds was selected to investigate if different categories of sound will influence the visual representation. All sounds can be accessed online at \url{https://bit.ly/3ta6crU} together with the sketches drawn by participants during the experiment.

\subsubsection{Study Two}\label{subsubsec:material_two}
The study was conducted entirely online. An initial audio check as well as a mid-study attention test ensured that participants listened to the audio playback either through headphones or speakers. Twenty different FM synthesiser sounds were selected from the dataset by \citeA{hayes2020there} discussed in Section~\ref{subsec:musical_timbre}. All sounds have a short attack and high sustain to ensure that participants focus on timbre rather than amplitude envelope. The FM synthesizer was implemented with the AudioWorklets interface of the Web Audio API to synthesise sound directly in the browser rather than using audio samples which enabled continuous playback of a sound without looping. All sounds were pitched to the MIDI note A3 and loudness-normalised. All sounds can be accessed online at \url{https://sfrl.github.io/study2_gallery/} together with the sketches drawn by participants during the experiment.

\subsection{Interfaces}\label{subsec:interfaces}
For both studies, digital sketching interfaces were used that can run in an internet browser. This makes it possible to collect sketches directly as sequential data as presented in Section~\ref{subsec:feature_analysis} and collect data online. The interface was developed from a generic to a specialised design through an iterative design process that sought to homogenise sketches between participants without majorly impacting their perceived sense of expressiveness. 

\subsubsection{Study One}\label{subsubsec:interface_one}

\begin{table*}[h!]
 \begin{center}
 \begin{tabular}{|p{0.5\linewidth}|p{0.5\linewidth}|}
%  Study One
\hline
    \textbf{Screenshot}
        & 
    \textbf{Description}   
    \\
\hline
\vspace{0cm}
    \includegraphics[width=0.5\linewidth]{Images/Interface_Study1.png}

    & 
    Generic design that allows for fixed-width, white strokes on a black canvas with a fixed size of 750x750 pixels. There are no sketch length limits, stroke simplification and undo or reset options. Sketch points are connected by straight lines.
   
    \\
 \hline
\end{tabular}
\end{center}
\vspace*{-0.05in}
 \caption{Interface for Study One}
 \label{tab:interface_1_specs}
\end{table*}

The requirements for this interface were that participants can only express their ideas through monochromatic strokes, but are not otherwise limited or supported. The black canvas was chosen to clearly distinguish the sketching area from the rest of the website. No option to partially or fully erase a sketch was provided so that participants would not revise their original idea. While the goal of Study One was to gain insights into the different sketched representations of sound, it also provided feedback on how to develop the interface for sound-sketching tasks. Feedback evaluation showed that, overall, participants were unsatisfied with the interface, especially in regards to sketching straight lines or smooth transitions. In addition, the canvas proved to be too small with multiple participants sketching over the edges. The black background received positive comments, but for some participants, it seemed to have artistic meaning rather than purely functional. While the design was successful in forcing participants to stick with their original idea, it did not take into consideration that participants might want to correct technical mistakes rather than change their idea.

\subsubsection{Study Two}\label{subsubsec:interface_two}

\begin{table*}[h!]
 \begin{center}
 \begin{tabular}{|p{0.5\linewidth}|p{0.5\linewidth}|}
%  Study Two
\hline
    \textbf{Screenshot}
        & 
    \textbf{Description}   
    \\
\hline
\vspace{0cm}
    \includegraphics[width=1\linewidth]{Images/Interface_Study2.png}

    & 
    Specialised design that allows for fixed-width, black strokes on a white canvas that scales to a participant's browser window (a minimal size of 800x600 pixels was enforced). The sketch length is limited to the range of 30 to 150 points. A sketch can only be submitted if it exceeds the lower length limit.  If a participant continues sketching after reaching the upper limit, sketch points are erased from the start. A small meter in the top left corner of the canvas indicates the current stroke length. Participants can reset the canvas to start over. The Ramer-Douglas-Peucker algorithm~\cite{rdp} was used for stroke simplification and stroke points were connected with Catmul-Rom splines~\cite{splines}.
    \\
 \hline
\end{tabular}
\end{center}
\vspace*{-0.05in}
 \caption{Interface for Study One}
 \label{tab:interface_2_specs}
\end{table*}

The feedback from Study One was addressed in the further interface design. Stroke simplification and spline interpolation were implemented to make it easier to sketch straight lines and smooth transitions. The canvas automatically stretches to the size of the browser window to prevent sketches from extending over the edges. A reset button ensures that participants can start over, but an erase or undo function was withheld to prevent participants from focusing on retouching their representations. These design choices are similar to the \textit{Quick, Draw!} interface introduced in Section~\ref{subsec:sketch_recognition} which makes collected sketches compatible with their dataset and deep-learning architecture. A major difference between the two interfaces is the length limit introduced in Study Two. The goal of this research is to collect sketches that encode sound characteristics through shape which typically results in simple, abstract representations. However, the categorisation of Study One sketches, as described in detail in Section~\ref{subsec:sketch_categorisation}, show a significant number of figurative representations like scenes or objects. As these sketches are on average longer, limiting the length permitted by the interface was expected to reduce those types of representations. This was tested in three small-scale design studies with 10-15 participants each. Different designs were evaluated on how well they guided participants towards simple, abstract representations while maintaining the feeling that the interface allows them to be expressive. This concluded in the setup described in Table~\ref{tab:interface_2_specs}.

\subsection{Participants}\label{subsec:participants}
Recruitment was open to all adults. The aim of these studies was to find out about sound-shape associations by sampling from the general population. As engagement with music was expected to have an influence on representations, for Study One musicians and non-musicians were recruited in equal parts.  

\subsubsection{Study One}\label{subsubsec:participants_one}
Twenty-eight participants were recruited through mailing lists and in person at the School of Electronic Engineering and Computer Science at Queen Mary University of London. This group was divided equally by gender (14 female, 14 male), 25 were adults below the age of 33 (three between 34 and 49), 22 had a Western background (16 from Europe, 4 from North America, 2 from South America) and 5 an Eastern background (4 from China, 1 from India) with one participant preferring not to disclose this information. A participant was defined as a musician if they responded to having at least one year of formal music education and engaged in musical activity (playing an instrument, producing/composing music) at least once a month. This resulted in 14 musicians and 14 non-musicians.

\subsubsection{Study Two}\label{subsubsec:participants_two}
Eighty-eight participants were recruited through the online platform Prolific.\footnote{\url{https://prolific.co/}} The majority of participants were female (48 female, 38 male, 2 other) and 84 were aged between 18 and 33 ($\mu$ = 22.5, $\sigma$ = 4.6). All participants had a Western background (56 from North America, 16 from Europe, 13 from South Africa and 3 from South America) with the majority coming from Mexico (54 participants). Sixty-six were students and 28 stated to have had at least one year of formal music education and actively engage in musical activity at least once a month.

\subsection{Procedure}\label{subsec:procedure}
A similar procedure was used for both studies. As Study Two was conducted online a more robust digital environment was deployed to ensure that participants completed the study successfully.  

\subsubsection{Study One}\label{subsubsec:procedure_one}
\begin{figure}[h]
\centering
\includegraphics[width=0.5\columnwidth]{Images/study1_setup.png}
\caption{Participant sketching a sound in Study One\label{fig:study1_procedure}}
\end{figure}

Participants first completed a questionnaire collecting demographic data and information about their experience with music. Participants were then asked to familiarise themselves with the sketching interface without audio before they were presented with the sound stimuli. The study intended to encourage a spontaneous response, therefore no information about the range of sounds was provided and participants were instructed to sketch what they believed to best represent each sound stimulus. Looped playback started automatically with the option to pause and resume. Each sound was played twice in a randomised order resulting in a total of twenty sketches per participant. After completion, a short semi-structured interview was conducted asking participants how they approached the task and whether they found it difficult. No time limit was given and the study typically took twenty to thirty minutes to complete. The study setup can be accessed online at \url{https://bit.ly/3j3FkVO}.

\subsubsection{Study Two}\label{subsubsec:procedure_two}
Participants were first presented with a set of information ensuring that they use a laptop or desktop device and are able to listen to sound either through headphones or loudspeakers. This was followed by a short introduction of the study that included guidelines on representing sounds in an abstract rather than figurative way, a short explanation of the sketching interface, a guide to adjust playback volume to a comfortable level and a check that the browser window size was at least 800$\times$600 pixels. Before starting the main task, participants were given the opportunity to familiarise themselves with the interface in two test rounds in which they were asked to sketch their associations with an imagined calm and noisy sound. For the main task, sound stimuli were played back automatically in a randomised order with the instruction \textit{Listen to the sound and draw your association}. An attention test was played back after completing half of the task asking participants to sketch the number four instead of a sound. The study concluded with a survey asking participants  about their experience with the study and the interface and collecting demographic data, experience with music and the hardware they used to complete the study. The study setup can be accessed online at \url{https://sketching-sounds.web.app/}. 

% \begin{figure}[h]
% \centering
% \includegraphics[width=0.5\columnwidth]{Images/sketch_guidance.png}
% \caption{Example sketches for abstract and figurative sketches that were shown to participants in Study Two.\label{fig:study2_guidance}}
% \end{figure}


\section{Sound and sketch feature extraction}\label{sec:extracted_features}

Table~\ref{tab:audiofeatures} shows the values of extracted audio features for Study One and Study Two.

\begin{table}[h]
\begin{subtable}[h]{\linewidth}

\resizebox{\textwidth}{!}{

\begin{tabularx}{1.5\textwidth}{
 l >{\raggedleft\arraybackslash}p{0.0833\linewidth}
 >{\raggedleft\arraybackslash}p{0.0833\linewidth}
 >{\raggedleft\arraybackslash}p{0.0833\linewidth}
 >{\raggedleft\arraybackslash}p{0.0833\linewidth}
 >{\raggedleft\arraybackslash}p{0.0833\linewidth}
 >{\raggedleft\arraybackslash}p{0.0833\linewidth}
 >{\raggedleft\arraybackslash}p{0.0833\linewidth}
 >{\raggedleft\arraybackslash}p{0.0833\linewidth}
 >{\raggedleft\arraybackslash}p{0.0833\linewidth}
 >{\raggedleft\arraybackslash}p{0.0833\linewidth}
 >{\raggedleft\arraybackslash}p{0.0833\linewidth}
 >{\raggedleft\arraybackslash}p{0.0833\linewidth}
}

{} &  Flatness Mean &  Centroid Mean &  RMS Mean &  RMS Slope &  Zero Crossing Mean &  Hardness &  Depth &  Brightness &  Roughness &  Warmth &  Sharpness &  Boominess \\
\midrule
Crackles         &  $4.1 * 10^{-2}$ &           1728 &     0.032 &          21 &               0.035 &        55 &     28 &          60 &         52 &      41 &         48 &         17 \\
Telephonic       &  $1.0 * 10^{-3}$ &            543 &     0.157 &           5 &               0.013 &        34 &     67 &          43 &         50 &      54 &         28 &         41 \\
Strings          &  $3.9 * 10^{-5}$ &           1042 &     0.151 &          10 &               0.020 &        42 &     57 &          52 &         52 &      54 &         35 &         33 \\
String Grains    &  $1.9 * 10^{-3}$ &           1177 &     0.107 &          22 &               0.031 &        52 &     40 &          56 &         56 &      49 &         37 &         25 \\
Subbass          &  $1.5 * 10^{-8}$ &            206 &     0.416 &           1 &               0.002 &        34 &     77 &          36 &         47 &      65 &         34 &         47 \\
Noise            &  $2.9 * 10^{-1}$ &           7915 &     0.086 &          34 &               0.144 &        81 &     44 &          81 &         81 &      29 &         68 &         22 \\
Piano            &  $4.5 * 10^{-7}$ &            542 &     0.051 &           1 &               0.013 &        42 &     65 &          49 &         40 &      54 &         31 &         42 \\
Impact           &  $2.8 * 10^{-4}$ &           1047 &     0.097 &           8 &               0.011 &        65 &     77 &          60 &         49 &      51 &         49 &         39 \\
Processed Guitar &  $1.7 * 10^{-3}$ &            229 &     0.344 &           3 &               0.002 &        30 &     80 &          39 &         40 &      63 &         37 &         46 \\
Electric Guitar  &  $4.5 * 10^{-6}$ &           1295 &     0.305 &           5 &               0.022 &        47 &     56 &          56 &         56 &      50 &         42 &         34 \\
\bottomrule
\end{tabularx}
}
\label{subtab:study1_audiofeatures}
\caption{Study One}
\end{subtable}

\begin{subtable}[h]{\linewidth}
\resizebox{\textwidth}{!}{%
\begin{tabularx}{1.5\textwidth}{
l >{\raggedleft\arraybackslash}p{0.0833\linewidth}
>{\raggedleft\arraybackslash}p{0.0833\linewidth}
>{\raggedleft\arraybackslash}p{0.0833\linewidth}
>{\raggedleft\arraybackslash}p{0.0833\linewidth}
>{\raggedleft\arraybackslash}p{0.0833\linewidth}
>{\raggedleft\arraybackslash}p{0.0833\linewidth}
>{\raggedleft\arraybackslash}p{0.0833\linewidth}
>{\raggedleft\arraybackslash}p{0.0833\linewidth}
>{\raggedleft\arraybackslash}p{0.0833\linewidth}
>{\raggedleft\arraybackslash}p{0.0833\linewidth}
>{\raggedleft\arraybackslash}p{0.0833\linewidth}
>{\raggedleft\arraybackslash}p{0.0833\linewidth}}
\toprule
{} &  Flatness Mean &  Centroid Mean &  RMS Mean &  RMS Slope &  Zero Crossing Mean &  Hardness &  Depth &  Brightness &  Roughness &  Warmth &  Sharpness &  Boominess \\
\midrule
Synth 1 & $4.9 * 10^{-8}$ &           562 &   0.0429 &         0 &             0.0199 &       34 &    42 &         39 &        29 &     39 &        29 &        21 \\
Synth 2 & $1.4 * 10^{-7}$ &           794 &   0.0614 &         0 &             0.0239 &       52 &    65 &         54 &        53 &     50 &        37 &        34 \\
Synth 3 & $3.8 * 10^{-5}$ &          2889 &   0.0266 &         0 &              0.113 &       67 &    42 &         75 &        72 &     30 &        61 &         9 \\
Synth 4 & $5.5 * 10^{-3}$ &          9670 &   0.0529 &         0 &              0.437 &       73 &    31 &         86 &        83 &     17 &        77 &        -6 \\
Synth 5 & $1.6 * 10^{-8}$ &           247 &   0.0731 &         0 &               0.01 &       32 &    68 &         30 &        41 &     53 &        25 &        43 \\
Synth 6 & $5.9 * 10^{-3}$ &          8607 &   0.0374 &         0 &              0.355 &       76 &    34 &         84 &        77 &     20 &        76 &        12 \\
Synth 7 & $6.0 * 10^{-6}$ &          2671 &    0.116 &         0 &               0.11 &       64 &    43 &         75 &        72 &     31 &        59 &        13 \\
Synth 8 & $2.7 * 10^{-6}$ &           327 &   0.0893 &       0 &             0.0116 &       11 &    62 &         30 &        42 &     48 &        23 &        40 \\
Synth 9 & $8.4 * 10^{-7}$ &          1693 &    0.104 &         0 &             0.0583 &       63 &    46 &         67 &        70 &     39 &        48 &        25 \\
Synth 10 & $5.9 * 10^{-7}$ &          1259 &    0.057 &         0 &             0.0511 &       48 &    45 &         65 &        68 &     40 &        40 &        23 \\
Synth 11 & $1.5 * 10^{-7}$ &           259 &      0.1 &       0 &            0.00995 &        3 &    64 &         27 &         0 &     56 &        26 &        44 \\
Synth 12 & $3.5 * 10^{-5}$ &          5493 &   0.0577 &         0 &              0.192 &       50 &    33 &         79 &        57 &     20 &        72 &         4 \\
Synth 13 & $7.8 * 10^{-4}$ &          8715 &   0.0539 &         0 &              0.357 &       61 &    30 &         84 &        63 &     31 &        80 &        25 \\
Synth 14 & $4.7 * 10^{-8}$ &           432 &   0.0466 &         0 &             0.0157 &       33 &    55 &         36 &        45 &     46 &        25 &        32 \\
Synth 15 & $1.8 * 10^{-7}$ &           515 &   0.0578 &         0 &             0.0194 &       39 &    61 &         46 &        56 &     52 &        29 &        33 \\
Synth 16 & $1.5 * 10^{-5}$ &          2768 &   0.0521 &         0 &              0.121 &       55 &    37 &         73 &        56 &     30 &        62 &         4 \\
Synth 17 & $9.6 * 10^{-3}$ &          9683 &    0.124 &         0 &              0.416 &       77 &    38 &         86 &        82 &     17 &        76 &        -1 \\
Synth 18 & $4.1 * 10^{-8}$ &           613 &   0.0758 &         0 &             0.0187 &       35 &    56 &         48 &        51 &     45 &        30 &        33 \\
Synth 19 & $8.2 * 10^{-7}$ &           569 &   0.0574 &         0 &               0.02 &       54 &    60 &         48 &        59 &     52 &        29 &        33 \\
Synth 20 & $2.6 * 10^{-3}$ &          6691 &   0.0564 &         0 &              0.259 &       69 &    25 &         82 &        65 &     25 &        74 &        15 \\
\bottomrule
\end{tabularx}}
\label{tab:study2_audiofeatures}
\caption{Study Two}
\end{subtable}
\caption{Sound features extracted from sound stimuli of both studies. For Librosa features, the mean values of all windows are reported.}
\label{tab:audiofeatures}
\end{table}

Table~\ref{tab:sketchfeatures} show the values of extracted sketch features for Study One and Study Two.

\begin{table}[h]
\begin{subtable}[h]{\linewidth}
\resizebox{\textwidth}{!}{%
\begin{tabularx}{1.5\textwidth}{
l >{\raggedleft\arraybackslash}p{0.1\linewidth}
>{\raggedleft\arraybackslash}
>{\raggedleft\arraybackslash}p{0.1\linewidth}
>{\raggedleft\arraybackslash}p{0.1\linewidth}
>{\raggedleft\arraybackslash}p{0.1\linewidth}
>{\raggedleft\arraybackslash}p{0.1\linewidth}
>{\raggedleft\arraybackslash}p{0.1\linewidth}
>{\raggedleft\arraybackslash}p{0.1\linewidth}
>{\raggedleft\arraybackslash}p{0.1\linewidth}
>{\raggedleft\arraybackslash}p{0.1\linewidth}
>{\raggedleft\arraybackslash}p{0.1\linewidth}}
\toprule
{} &  Number of Strokes &  Average Length [px] &  Average Time [ms] &  Average Speed [px/ms] &  Inter-sections [1/100px] &  Narrow Curves [1/100px] &  Wide Curves [1/100px] &  Obtuse Angles [1/100px] &  Right Angles [1/100px] &  Acute Angles [1/100px] \\
\midrule
Crackles         &               10.5 &                         471 &                      2796 &                   0.22 &                     1.78 &                     0.55 &                   0.60 &                     0.94 &                    0.22 &                    0.67 \\
Telephonic       &                6.0 &                        1091 &                      4613 &                   0.27 &                     1.05 &                     0.16 &                   0.37 &                     0.55 &                    0.08 &                    0.26 \\
Strings          &                3.9 &                         900 &                      4379 &                   0.26 &                     0.31 &                     0.10 &                   0.23 &                     0.28 &                    0.06 &                    0.09 \\
String Grains    &               10.1 &                         914 &                      3340 &                   0.28 &                     1.48 &                     0.24 &                   0.21 &                     0.58 &                    0.15 &                    0.54 \\
Subbass          &                4.5 &                        1328 &                      6154 &                   0.23 &                     0.92 &                     0.17 &                   0.43 &                     0.51 &                    0.08 &                    0.30 \\
Noise            &               12.8 &                        1816 &                      3540 &                   0.57 &                     2.37 &                     0.32 &                   0.22 &                     0.61 &                    0.16 &                    0.56 \\
Piano            &                3.8 &                         586 &                      3020 &                   0.29 &                     0.61 &                     0.06 &                   0.31 &                     0.20 &                    0.02 &                    0.06 \\
Impact           &                7.2 &                        1239 &                      3313 &                   0.51 &                     1.12 &                     0.14 &                   0.28 &                     0.24 &                    0.05 &                    0.31 \\
Processed Guitar &                4.3 &                        1124 &                      4707 &                   0.33 &                     0.43 &                     0.17 &                   0.23 &                     0.49 &                    0.09 &                    0.22 \\
Electric Guitar  &                5.3 &                        1121 &                      4561 &                   0.33 &                     0.93 &                     0.12 &                   0.12 &                     0.35 &                    0.08 &                    0.40 \\
\bottomrule

\end{tabularx}}

\label{subtab:study1_sketchfeatures}
\caption{Study One}
\end{subtable}
\begin{subtable}[h]{\linewidth}
\resizebox{\textwidth}{!}{%
\begin{tabularx}{1.5\textwidth}
{l >{\raggedleft\arraybackslash}p{0.1\linewidth}
>{\raggedleft\arraybackslash}
>{\raggedleft\arraybackslash}p{0.1\linewidth}
>{\raggedleft\arraybackslash}p{0.1\linewidth}
>{\raggedleft\arraybackslash}p{0.1\linewidth}
>{\raggedleft\arraybackslash}p{0.1\linewidth}
>{\raggedleft\arraybackslash}p{0.1\linewidth}
>{\raggedleft\arraybackslash}p{0.1\linewidth}
>{\raggedleft\arraybackslash}p{0.1\linewidth}
>{\raggedleft\arraybackslash}p{0.1\linewidth}
>{\raggedleft\arraybackslash}p{0.1\linewidth}}
\toprule
{} & Number of Strokes & Average Length [px] & Average Time [ms] & Average Speed [px/ms] & Intersections [1/100px] & Narrow Curves [1/100px] & Wide Curves [1/100px] & Obtuse Angles [1/100px] & Right Angles [1/100px] & Acute Angles [1/100px] \\
\midrule
Synth 1 &             3.4 &                1002 &              3021 &                  0.44 &                     1.8 &                     0.3 &                  0.84 &                     1.1 &                   0.13 &                   0.31 \\
Synth 2 &             3.1 &                 876 &              2923 &                   0.4 &                     2.6 &                    0.55 &                  0.96 &                     1.1 &                   0.24 &                   0.53 \\
Synth 3 &             2.9 &                1186 &              3274 &                  0.46 &                     2.0 &                    0.42 &                  0.88 &                     1.1 &                  0.079 &                    1.2 \\
Synth 4 &             3.9 &                2125 &              3178 &                  0.74 &                     5.4 &                    0.32 &                  0.39 &                    0.86 &                   0.23 &                    1.4 \\
Synth 5 &             3.0 &                1636 &              3239 &                  0.53 &                     1.2 &                     0.9 &                   1.2 &                     1.7 &                   0.53 &                   0.62 \\
Synth 6 &             4.3 &                1444 &              3344 &                  0.49 &                     3.7 &                    0.75 &                  0.67 &                     1.4 &                   0.19 &                    1.6 \\
Synth 7 &             3.9 &                3644 &              2725 &                   1.2 &                     5.8 &                     0.5 &                  0.54 &                     1.0 &                   0.19 &                    1.5 \\
Synth 8 &             2.03 &                 936 &              3666 &                  0.31 &                    0.74 &                    0.37 &                  0.84 &                    0.58 &                  0.096 &                   0.21 \\
Synth 9 &             5.2 &                1815 &              2752 &                  0.79 &                     5.7 &                    0.42 &                  0.98 &                    0.95 &                   0.19 &                    0.7 \\
Synth 10 &             3.9 &                1555 &              3008 &                  0.56 &                     2.2 &                    0.45 &                   1.0 &                     1.6 &                   0.18 &                   0.48 \\
Synth 11 &             2.1 &                 834 &              3682 &                  0.27 &                     0.6 &                    0.19 &                   1.0 &                    0.92 &                  0.082 &                   0.12 \\
Synth 12 &             2.6 &                1352 &              3533 &                  0.46 &                     2.9 &                    0.34 &                  0.61 &                    0.68 &                   0.12 &                   0.63 \\
Synth 13 &             3.8 &                1239 &              3125 &                  0.48 &                     3.2 &                    0.41 &                  0.68 &                    0.81 &                   0.14 &                    0.6 \\
Synth 14 &             2.5 &                 995 &              3181 &                  0.37 &                    0.97 &                     0.5 &                   0.9 &                    0.78 &                  0.045 &                    0.2 \\
Synth 15 &             2.8 &                1543 &              2984 &                  0.55 &                     3.3 &                    0.62 &                  0.74 &                     1.5 &                   0.31 &                    1.4 \\
Synth 16 &             2.7 &                1104 &              2816 &                  0.48 &                     3.1 &                    0.63 &                  0.78 &                     1.1 &                   0.18 &                   0.72 \\
Synth 17 &             2.3 &                3055 &              3171 &                   1.1 &                     6.4 &                    0.34 &                  0.24 &                    0.58 &                   0.15 &                    1.2 \\
Synth 18 &             2.6 &                1269 &              3555 &                  0.39 &                     2.0 &                    0.35 &                  0.68 &                    0.72 &                   0.11 &                   0.34 \\
Synth 19 &             3.3 &                1854 &              3279 &                  0.54 &                     3.8 &                    0.83 &                  0.89 &                     1.6 &                   0.31 &                    1.4 \\
Synth 20 &             4.1 &                1993 &              2961 &                  0.67 &                     3.2 &                    0.26 &                  0.54 &                    0.71 &                   0.12 &                   0.59 \\
\bottomrule

\end{tabularx}}

\label{subtab:study2_sketchfeatures}
\caption{Study Two}
\end{subtable}
\caption{Sketch features for both studies. The mean value from all participants is presented for each sound stimulus.}
\label{tab:sketchfeatures}
\end{table}

\section{Qualitative feedback analysis}\label{sec:interview_analysis}
Qualitative feedback was given in a semi-structured interview in Study One which provided a broad picture into participants' approaches and laid the ground work for the development of Study Two. In Study Two, brief qualitative feedback was given in written form and supported by quantitative survey responses. This section first focuses on feedback about the task itself and then summarises feedback about the interaction with the interface.   

\begin{figure}[h]
\centering
\includegraphics[width=0.5\columnwidth]{Images/worldcloud.png}
\caption{Reoccurring themes when describing sound extracted from participant interviews in Study One.\label{fig:study1_wordcloud}}
\end{figure}

% Easily recognisable sounds like \textit{Piano} and \textit{Strings} were typically referred to by the instrument that produced them. 
% For other sounds, either descriptors closer to the sound's characteristic like `crackly',`buzzy' or `noise' or music technology vocabulary like `amplitude',`frequency' or `timbre' were chosen. Some participants appeared to pay increased focus to the temporal evolution of a sound represented in the descriptors `movement' or `spatial'.

\subsection{Sketching task}
Task difficulty was reported as easy/neutral/hard by 14/6/8 participants in Study One and 74/8/6 participants in Study Two. Despite the positive skew in Study Two, mixed responses were recorded to the question of whether it was easy to think about sound in a visual way with 44/20/24 agree/neutral/disagree which is well captured in the response `I have never had to interpret sounds visually, therefore I found it to be kind of a difficult but interesting task.' (P2.70). This was echoed in interviews from Study One where some participants found it difficult to `think of sound in a very visual way' (P1.8) (P1.16).  However, for some it became easier to visualise a sound once the task started, as one participant stated `I wasn't really expecting to be able to visualize sound, but some of those frequencies were extremely clear to me, as far as how they looked in my brain.' (P2.53). Participants who felt that the task was easy thought that `there was no right or wrong [answer]' (P1.4), `it was just about being creative' (P1.15), they did not have to `achieve something' (P1.10), the setup `allowed the listener space to interpret all sorts of sound visually' (P2.91) or simply found the task `interesting' (P2.39, P2.41, P2.70, P2.83, P2.91) and `fun' (P2.20, P2.33, P2.45). For Study Two, some criticized that the `sounds were very alike' P(2.13) which made it `[...] hard to find an specifically draw[ing] to each sound' P(2.34). On the contrary, in Study One some participants struggled with the `great variety in the sounds' (P1.2) which made it difficult to find a consistent approach. While some participants approached the task as an intuitive, creative activity, others were concerned with establishing a consistent visual language, difficulties arose while deciding which sound characteristics to follow because `there are too many things to consider' like `brightness or aggressiveness or how it [timbre] develops over time' (P1.6).  Deploying a more systematic rather than intuitive approach appeared more difficult with one participant who did not deviate from their initial concept finding themselves `going round in circles, and question how valid the whole approach is' (P1.9). Some participants reported that `complicated ones [sounds] sounded like pictures, and then the simple ones [...] like piano notes were a lot harder to draw' (P1.8) possibly because they `hear [them] all the time' (P1.1), while other participants thought that `itâ€™s pretty straightforward because I know a piano note more than others' (P1.5). Most participants approached the task by listening to `the actual sonic qualities of them [the sounds]' (P1.1) and representing them with `[...]abstract patterns, along with patterns that would come from what I thought the instruments were, and it was kind of a mixture of going back and forth between the two.' P(1.5). Familiar sounds like the piano can influence participants to choose figurative representations that include the sound-producing source. However, some participants adopted a figurative approach that does not pay attention to specific sound characteristics but rather extracts general information like an emotion from a sound and then depicts a scene or an object that fits this information. One participant explained `I wanted to show the emotion in the sound like the sound was scary so I drew the forest with only one person inside it. If I think the sound is peaceful, I will draw the sea and the sun and the water.' (P1.21). These two different approaches suggest that abstract representations are more strongly informed by sound-shape associations and figurative representations draw more strongly from an emotional or memory response. 

\begin{figure}[h]
\centering
\includegraphics[width=0.3\columnwidth]{Sketches/Object_pt21snd1.png}
\caption{A participant in Study One represented a sound that they perceived as `scary' with a scary scene.  \label{fig:scary_scene}}
\end{figure}

\subsection{Sketching interface}
Analysing responses for feedback about the interface interaction showed that for Study One, overall, participants were unsatisfied with the interface with twelve mentioning that they would prefer a pen (either digital or analogue) to be able to sketch more accurately. Six stated that they would have liked to utilise additional visual tools like color, different strokes and textures. However, responses suggested that while the interface `could be more expressive [...] for the purpose it was expressive
enough' (P1.5). The feedback lead to the overhaul of the interface design for Study Two described in Section~\ref{subsubsec:interface_two}. For Study Two, 70 and 75 participants responded with agree or completely agree to the questions \textit{I thought the drawing interface allowed me to be expressive.} and \textit{I thought the drawing interface was easy to use.}. Three participants still mentioned that they would like to add colors to the interface and one mentioned that the task `would be easier to do on my phone' (P2.85). A further three participants mentioned that they would like to increase or discard the stroke length limit.   

\section{Sketch Categorisation}\label{sec:sketch_categorisation}
From the interview analysis of Study One two broad representational approaches can be defined: an abstract approach that is guided at least in part by sound-shape associations and a figurative approach that is strongly influenced by imagery evoked by a sound. Sketch categorisation for Study One focuses on further formalizing these findings and investigate how participants could be guided towards abstract representations. Study Two focuses on the influence of prominent sound characteristics on abstract representational categories. 

\subsection{Study One: manual categorisation through card sorting}\label{subsec:categorisation_one}
As described in Section~\ref{sec:sketch_categorisation}, sketches were categorised in an open card sorting study. Analysis of the responses returned an optimal number of five categories that were named: \textit{Chaotic/Jagged} (172 sketches), \textit{Radiating/Round} (126), \textit{Lines} (120), \textit{Objects/Scenes} (86) and \textit{Grains} (56). The results are visualised in Figure~\ref{fig:study_one_categories}. Descriptive keywords and sketch examples for each category can be found in Table \ref{tab:study1_sketches}. A maximal silhouette coefficient of 0.49 suggests that categories are distinguishable, but not clearly separated which is also reflected by occasionally overlapping keywords. The \textit{Object/Scenes} category consists mainly of figurative representations while the remaining categories include mainly abstract representations. Chi-squared test suggests that non-musicians produce \textit{Objects/Scenes} sketches more often (${\chi}^2$(1,N$=$28)$=$22.51 p$<$.0001) while musicians produce \textit{Lines} sketches at a higher rate (${\chi}^2$(1,N$=$28)$=$7.5 p$<$.01) possibly because this category contains sketches that appear to reference audio visualisations like envelopes or waveforms. Category counts for \textit{Objects/Scenes} sketches significantly differ between sounds (${\chi}^2$(9)=67.07 p$<$.0001) with post-hoc analysis revealing that \textit{Piano} and \textit{Impact} show significantly higher counts than \textit{Noise}, \textit{String Grains} and \textit{Processed Guitar} (p$<$.01 for each pair). A possible explanation is that \textit{Piano} and \textit{Impact} have an easily identifiable source that participants attempted to sketch rather than capturing sound characteristics directly. \textit{Noise} and \textit{String Grains} that show high values for the \textit{roughness} audio feature (81 and 56) as displayed in Table~\ref{subtab:study1_audiofeatures} also have the largest share of sketches in the \textit{Chaotic/Jagged} category. On the other hand, \textit{Subbass}, \textit{Telephonic} and \textit{Impact} with high values for the \textit{warmth} audio feature (65, 54 and 51) have the highest share of the \textit{Radiating/Round} category.  Interestingly, despite high values for \textit{warmth} (54), \textit{Piano} and \textit{Strings} are more frequently represented with \textit{Lines} sketches, possibly because they are comparably spectrally simple sounds which is reflected by the low values for spectral flatness. Table~\ref{tab:study1_sketch_lengths} shows that \textit{Object/Scenes} has the highest average number of sketch points and second highest average number of strokes. The highest number of strokes can be found in the \textit{Grains} category which is most prevalent in the sounds \textit{Crackles} and \textit{String Grains}. However, with a low average number of points this category appears to represent the intersecteness of sound quantified by the audio feature \textit{RMS slope} with multiple short, simple structures. \textit{Chaotic/Jagged} and \textit{Radiating} have a similar average number of points as \textit{Object/Scenes}, but a significantly lower number of average strokes. This analysis indicates that \textit{Object/Scenes} sketches are more likely to be long and complex with multiple components which informed the interface development for Study Two described in Section~\ref{subsubsec:interface_two}. 

\begin{figure}[h]
\centering
\includegraphics[width=0.9\columnwidth]{Images/study1_clustered_sketches.png}
\caption{Study One sound sketches organised by the K-Means clusters calculated from the card-sorting study. Colours indicate the different clusters.}\label{fig:study_one_categories}
\end{figure}

\begin{table*}[t]
 \begin{center}
 \begin{tabular}{|p{0.07\linewidth}|p{0.07\linewidth}||p{0.07\linewidth}|p{0.07\linewidth}||p{0.07\linewidth}|p{0.07\linewidth}||p{0.07\linewidth}|p{0.07\linewidth}||p{0.07\linewidth}|p{0.07\linewidth}|}
  
    \multicolumn{2}{l}{\textbf{Grains}} 
    & 
    \multicolumn{2}{l}{\textbf{Lines}}
    & 
    \multicolumn{2}{l}{\textbf{Object/Scenes}} 
    & 
    \multicolumn{2}{l}{\textbf{Chaotic/Jagged}} 
    & 
    \multicolumn{2}{l}{\textbf{Radiating/Round}} 
    \\
    \hline
     \multicolumn{2}{|p{0.14\linewidth}||}{{\tiny \textit{Small, repeated, grainy, spots, multiple components, layers, abstract, distinct}}} 
    &
    \multicolumn{2}{|p{0.14\linewidth}||}{{\tiny \textit{round, soft, continuous, jagged, irregular, simple, single, lines}}}
    & 
    \multicolumn{2}{|p{0.14\linewidth}||}{{\tiny \textit{real-life objects, environment, actions or feelings, abstract structures}}} 
    & 
    \multicolumn{2}{|p{0.14\linewidth}||}{{\tiny \textit{chaotic, intense, jagged, multiple layers, single objects}}} 
    & 
    \multicolumn{2}{|p{0.14\linewidth}||}{{\tiny \textit{round, circular, spiral, sharp, shaking, distinct objects, radiating, natural}}}
    \\
  \hline
  \hline
    \includegraphics[width=1.05\linewidth]{Sketches/Grains_pt11snd6B.png} 
    &
    \includegraphics[width=1.05\linewidth]{Sketches/Grains_pt4snd1B.png} 
    &
    \includegraphics[width=1.05\linewidth]{Sketches/Lines_pt1snd2.png} 
    &
    \includegraphics[width=1.05\linewidth]{Sketches/Lines_pt2snd7B.png} 
    &
    \includegraphics[width=1.05\linewidth]{Sketches/Object_pt12snd8.png} 
    & 
    \includegraphics[width=1.05\linewidth]{Sketches/Object_pt21snd1.png} 
    & 
    \includegraphics[width=1.05\linewidth]{Sketches/Chaotic_pt10snd4.png} 
    & 
    \includegraphics[width=1.05\linewidth]{Sketches/Chaotic_pt15snd5B.png} 
    &
    \includegraphics[width=1.05\linewidth]{Sketches/Radiating_pt14snd8.png}
    &
    \includegraphics[width=1.05\linewidth]{Sketches/Radiating_pt28snd9.png}
  \\
  \hline
    \includegraphics[width=1.05\linewidth]{Sketches/Grains_pt26snd9.png} 
    & 
    \includegraphics[width=1.05\linewidth]{Sketches/Grains_pt8snd4.png} 
    & 
    \includegraphics[width=1.05\linewidth]{Sketches/Lines_pt22snd9.png} 
    & 
    \includegraphics[width=1.05\linewidth]{Sketches/Lines_pt5snd3B.png}
    & 
    \includegraphics[width=1.05\linewidth]{Sketches/Object_pt16snd7.png} 
    & 
    \includegraphics[width=1.05\linewidth]{Sketches/Object_pt23snd10.png} 
    & 
    \includegraphics[width=1.05\linewidth]{Sketches/Chaotic_pt13snd10.png} 
    & 
    \includegraphics[width=1.05\linewidth]{Sketches/Chaotic_pt3snd6.png} 
    & 
    \includegraphics[width=1.05\linewidth]{Sketches/Radiating_pt17snd6B.png}
    &
    \includegraphics[width=1.05\linewidth]{Sketches/Radiating_pt2snd5B.png}
    \\
  \hline
 \end{tabular}
\end{center}
\vspace*{-0.05in}
 \caption{ Sketch categories with examples. Category names and keywords were obtained through thematic analysis as described in Section \ref{sec:categorisation_analysis}. \textit{Objects/Scenes} mainly refers to real-world associations while other categories highlight different abstract approaches, but category clusters might overlap with a number of sketches showing characteristics of more than one category. Colours were inverted for better visibility.}
 \label{tab:study1_sketches}
\end{table*}

\begin{table*}[t]
 \begin{center}
 \begin{tabular}{|p{0.05\linewidth}||p{0.05\linewidth}|p{0.05\linewidth}||p{0.05\linewidth}|p{0.05\linewidth}||p{0.05\linewidth}|p{0.05\linewidth}||p{0.05\linewidth}|p{0.05\linewidth}||p{0.05\linewidth}|p{0.05\linewidth}|}
    &
    \multicolumn{2}{l}{\textbf{Grains}} 
    & 
    \multicolumn{2}{l}{\textbf{Lines}}
    & 
    \multicolumn{2}{l}{\textbf{Object/Scenes}} 
    & 
    \multicolumn{2}{l}{\textbf{Chaotic/Jagged}} 
    & 
    \multicolumn{2}{l}{\textbf{Radiating}} 
    \\
    \hline
    & {\tiny \textbf{Mean}} & {\tiny\textbf{Std}} & {\tiny\textbf{Mean}} & {\tiny\textbf{Std}} & {\tiny\textbf{Mean}} & {\tiny\textbf{Std}} & {\tiny\textbf{Mean}} & {\tiny\textbf{Std}} & {\tiny\textbf{Mean}} & {\tiny\textbf{Std}}
\\
  \hline
  {\tiny\textbf{Points}} & 560 & 532 & 534 & 338 & 1068 & 557 & 1060 & 696 & 914 & 722 
  \\
  \hline
    {\tiny\textbf{Strokes}} & 14 & 12 & 2 & 2 & 13 & 9 & 7 & 10 & 4 & 4
    \\
  \hline
 \end{tabular}
\end{center}
\vspace*{-0.05in}
 \caption{Average number of points and strokes for each sketch category rounded to closest integer. \textit{Object/Scenes} shows the highest number of points and second highest number of strokes which implies that this category could be reduced when limiting the size of a sketch.}
 \label{tab:study1_sketch_lengths}
\end{table*}


\begin{figure}[h]
\centering
\includegraphics[width=0.45\columnwidth]{Images/Categories_Sound.png}
\caption{Categories by sound stimulus in Study One.\label{fig:categories_by_sound}}
\end{figure}

\subsection{Study Two: automated categorisation using machine learning}\label{subsec:categorisation_two}
As described in Section~\ref{subsec:qualitative_analysis} the \textit{SketchRNN} deep learning architecture was used to create a latent representation of the collected sound-sketches. Figure~\ref{fig:study2_pca_vae} shows a visualisation of the latent space and categorisation through cluster analysis. In contrast to Study One, three clusters were determined to separate the data best, however a silhouette score of 0.36 shows that large overlaps exist between them. Three major categories informed by the card sorting annotations from Study One were defined: \textit{Lines} (610), \textit{Chaotic/Jagged} (455 sketches),  and \textit{Radiating/Round} (694). The categories \textit{Object/Scenes} and \textit{Grains} were no longer present in the dataset. This was expected as the interface design discouraged \textit{Objects/Scenes} sketches and \textit{Grains} sketches were largely found in intersected sounds that were not included in the audio stimuli for Study Two. Category counts for \textit{Chaotic/Jagged} and \textit{Lines} differ significantly between sound groups (${\chi}^2$(5)=20.38 p$<$.01 and ${\chi}^2$(5)=34.29 p$<$.00001), but no significant differences could be found for \textit{Radiating/Round} (${\chi}^2$(5)=9.59 p$>$.05). Post-hoc analysis showed a prominent differences in category distribution between the most and least rough sounds that can also be seen in Figure~\ref{fig:study2_categories_by_sound}. Following the procedure described in Section~\ref{subsec:qualitative_analysis}, the sounds \textit{Synth 9}, \textit{Synth 13} and \textit{Synth 19} were grouped for \textit{rough} and \textit{Synth 1}, \textit{Synth 11} and \textit{Synth 14} for \textit{not rough}. The \textit{rough} audio feature shown in Table~\ref{subtab:study2_sketchfeatures} was determined as 70, 63 and 59 for the \textit{rough} group and 29, 0 and 45 for \textit{not rough}. This confirms the findings from Study One that rough sounds are more frequently represented with chaotic, complex sketches and calmer, less rough sounds with simpler lines and the \textit{timbral model} by \citeA{pearceModellingTimbralHardness2019} provides a suitable measure for roughness. 

\begin{figure}[h]
\centering
\includegraphics[width=0.9\columnwidth]{Images/study2_vae_pca.png}
\caption{Study Two sound sketches organised in the latent space that was reduced to two dimensions with PCA. Colours indicate the different clusters found through K-Means.}\label{fig:study_two_categories}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.45\columnwidth]{Images/study2_categories_by_sound.png}
\caption{Categories by sounds grouped by attributes in Study Two.\label{fig:study2_categories_by_sound}}
\end{figure}

\section{Quantitative feature analysis}\label{sec:feature_analysis}
This analysis was conducted with the aim to statistically investigate to what extent sound-shape associations emerge from sound-sketches. First, inter-rater reliability was determined to confirm sketch features used in this research can measure agreement between participants. This was followed by calculating correlations between individual audio and visual features. As discussed in Section~\ref{subsec:feature_analysis}, interpretation of the results needs to consider that this analysis uses averaged values


\subsection{Inter-rater reliability}\label{subsec:inter-rater_reliability}
For Study One, reliability measures were good to excellent for \textit{Intersections} and \textit{Acute Angels}, poor to good for \textit{Average Speed} and moderate to good for all remaining features within the 95\% confidence interval (CI). For Study Two, measures were excellent for \textit{Acute Angels} and good to excellent for \textit{Intersections}, \textit{Wide Curves} and \textit{Right Angles} solidifying findings from Study One with higher reliability scores and narrower confidence intervals. In contrast to Study One, \textit{Number of Strokes} and \textit{Average Time} only returned poor to moderate reliability which can be accredited to the sketch length limit of the interface. \textit{Average Speed} however showed good to excellent reliability compared to poor to good in Study One implying that participants might have expressed characteristics through their drawing speed that would have been expressed through more complex sketches with an unrestricted interface. This is supported by a larger variance in average drawing speed compared to Study One ($\sigma$ = 0.23 compared to $\sigma$ = 0.12). These results suggest that some level of agreement exists between averaged participants on how to represent sounds visually and that it can be measured with the extracted sketch features.

\begin{figure}[!ht]
    \subfloat[Study One\label{fig:icc_one}]{%
      \includegraphics[width=0.45\textwidth]{Images/Study1_ICC.png}
    }
    \hfill
    \subfloat[Study Two\label{fig:icc_two}]{%
      \includegraphics[width=0.45\textwidth]{Images/Study2_ICC.png}
    }
    \caption{Mean values and 95\% CI of ICC(2,k) inter-rater reliabilities for each sketch feature with evaluation guidelines proposed by Koo and Li (df1$=$19, df2$=$513, p$<$.01 for all features)} 
    \label{fig:icc}
  \end{figure}

\subsection{Feature correlation}\label{subsec:feature_correlation}
Several significant correlations were found between sketch and audio features in both studies.  For Study One, \textit{Acute Angles} (11), \textit{Intersections} (9) and \textit{Number of Strokes} (8) show the highest statistically significant (p$<$.05) number of strong (r$>$.6) and very strong (r$>$.8) correlations with audio features. The strongest correlation overall was found between \textit{RMS Mean} and \textit{Average Time} (r$=$.95, p$<$.001). Opposing audio features like \textit{Warmth} and \textit{Sharpness} showed similar absolute correlation values but opposite directions for \textit{Number of Strokes}, \textit{Intersections} and \textit{Acute Angles}. For Study Two, \textit{Wide Curves} (9), \textit{Intersections} (7) and \textit{Acute Angles} (3) show the highest statistically significant (p$<$.05) number of strong (r$>$.6) and very strong (r$>$.8) correlations with audio features. The strongest correlation overall was found between \textit{Intersections} and \textit{Hardness} (r$=$.81, p$<$.001). While only 12 strong and very strong correlations were found compared to 19 in Study One, more results were significant at p$<$.05 level (49 compared to 37). Similar trends can be observed between both studies with \textit{Acute Angles} and \textit{Intersections} showing negative correlations with audio features \textit{Boominess}, \textit{Warmth} and \textit{Depth} and positive correlations with \textit{Roughness}, \textit{Brightness} and \textit{Hardness}. \textit{Average Speed} shows similar correlations to \textit{Number of Strokes} in Study One for example with \textit{Hardness} and \textit{Boominess} further supporting the hypothesis that drawing speed compensated for the sketch length restrictions. In contrast to Study One, \textit{Wide Curves} shows a large number of significant correlations with audio features that are mirroring \textit{Acute Angles} correlations as expected from known sound-shape associations. As all sound stimuli were created with the same amplitude envelope in Study Two the features \textit{RMS Slope} and \textit{RMS Mean} did not provide distinguishing descriptions and consequently did not show any significant correlations with sketch features. 
 
 \begin{figure}[!ht]
    \subfloat[Study One\label{fig:spearman_one}]{%
      \includegraphics[width=0.45\textwidth]{Images/Study1_Spearman.png}
    }
    \hfill
    \subfloat[Study Two\label{fig:spearman_two}]{%
      \includegraphics[width=0.45\textwidth]{Images/Study2_Spearman.png}
    }
    \caption{Spearman's rank correlation coefficients between sketch and audio features with annotated p-values: p$<$.05 (*), .01 (**), .001 (***)} 
    \label{fig:spearman}
  \end{figure}


% \section{Sketch Interface Design}
% \label{sec:interface_design}
% In this research, sound-sketches are created by participants with a digital sketching interface. The interface development followed an iterative design process where each iteration was evaluated through human participant studies~\cite{norman2013design}. Different evaluation methods were employed at different design stages, starting with behavioural observation and participant interviews in Study One, followed by three small-scale online prototype studies evaluated through A/B testing, usability surveys and clickstream data analysis informing the development of the interface used in Study Two. The aim was to find a configuration that participants perceived to be expressive and easy to use while subtly guiding them towards simple, abstract sketches rather than more complex depictions of real-life objects or scenes. In order to be easily accessible for participant,  the development focused on a system agnostic implementation that can run in a web-browser. The interface was built on \textit{p5.js}, a JavaScript library that allows for quick implementations of interactive web applications using the HTML5 canvas object. Participants can start a sketch by clicking and then dragging the cursor on the canvas. The looped \textit{draw()} function in \textit{p5.js} was set to refresh at a frame rate of 30 and on each loop collect the current clicked mouse \textit{x} and \textit{y} position and timestamp representing the milliseconds since starting a sketch. Lines of fixed width connect the individual stroke points. As this research focuses on sound representations through shapes, none of the interfaces allowed to manipulate other visual features like colour, texture or stroke width. Participants had to use a computer mouse or laptop touchpad for sketching.     



% The design process was divided into three stages. In the first stage, a simple prototype was developed for Study One. Based on the results of that study, a series of three small-scale studies was conducted that focused on the improvement of the interface for Study Two. An overview of all interface designs can be seen in Table~\ref{tab:interface_overview} and the evaluation results in Table~\ref{tab:interface_results}. 



% \subsection{Interface Study One}
% \label{sec:study1_interface_design}

% \begin{table*}[h!]
%  \begin{center}
%  \resizebox{\textwidth}{!}{
%  \begin{tabular}{p{0.73\linewidth}|p{0.33\linewidth}|p{0.23\linewidth}|}
% %  Study One
%     \cline{2-3}
%     \multirow{4}{\linewidth}{
%     \vspace{0cm}
%     \includegraphics[width=0.5\linewidth]{Images/Interface_Study1.png}

%     }
%     & 
%     \textbf{Evaluation}
%     &
%     \textbf{Participants}
%     \\
%     \cline{2-3}
    
%     &
%     Interface used in Study One (Section~\ref{sec:study1} and evaluated through participant observation and interviews.
%     &
%     28 (14 female, 14 male), median age category 26-33
   
%     \\
%     \cline{2-3}
    
%     &
%     \textbf{Usability}
%     & 
%     \textbf{Abstract}
%     \\
%   \cline{2-3}
    
%     &
%     1 median interface satisfaction (1-very unsatisfied to 5-very satisfied) quantified from interviews 
%     &
%     85{\%} (474/560 sketches)
%     \\
%   \cline{2-3}
 
% \end{tabular}}
% \end{center}
% \vspace*{-0.05in}
%  \caption{The first interface featured white strokes on a black background which was reversed for all following interfaces.     Fixed 750x750 pixel sketching canvas, unlimited points and strokes without simplification, no undo or reset options }
%  \label{tab:interface_1_specs}
% \end{table*}

% Study One described in Section~\ref{sec:study1} was conducted in person using the same MacBook for all participants. An overview of the interface can be seen in Table~\ref{tab:interface_1_specs}. Overall, participants were unsatisfied with the interface with twelve mentioning that they would prefer a pen (either digital or analogue) to be able to sketch more accurately. Six stated that they would have liked to utilise additional visual tools like colour, different strokes and textures. While the design was successful in forcing participants to stick with their original idea, it did not take into consideration that participants might want to correct technical mistakes rather than changing their idea. The black background received positive comments, but for some participants it seemed to have artistic meaning rather than being purely functional 'I felt like it matched the atmosphere of some of the sounds'(P1) or 'because there was a black canvas and white which is not typical'(P2).

% \begin{table*}[t]
%  \begin{center}
%  \begin{tabular}{|p{0.05\linewidth}||p{0.05\linewidth}|p{0.05\linewidth}||p{0.05\linewidth}|p{0.05\linewidth}||p{0.05\linewidth}|p{0.05\linewidth}||p{0.05\linewidth}|p{0.05\linewidth}||p{0.05\linewidth}|p{0.05\linewidth}|}
%     &
%     \multicolumn{2}{l}{\textbf{Grains}} 
%     & 
%     \multicolumn{2}{l}{\textbf{Lines}}
%     & 
%     \multicolumn{2}{l}{\textbf{Object/Scenes}} 
%     & 
%     \multicolumn{2}{l}{\textbf{Chaotic/Jagged}} 
%     & 
%     \multicolumn{2}{l}{\textbf{Radiating}} 
%     \\
%     \hline
%     & {\tiny \textbf{Mean}} & {\tiny\textbf{Std}} & {\tiny\textbf{Mean}} & {\tiny\textbf{Std}} & {\tiny\textbf{Mean}} & {\tiny\textbf{Std}} & {\tiny\textbf{Mean}} & {\tiny\textbf{Std}} & {\tiny\textbf{Mean}} & {\tiny\textbf{Std}}
% \\
%   \hline
%   {\tiny\textbf{Points}} & 560 & 532 & 534 & 338 & 1068 & 557 & 1060 & 696 & 914 & 722 
%   \\
%   \hline
%     {\tiny\textbf{Strokes}} & 14 & 12 & 2 & 2 & 13 & 9 & 7 & 10 & 4 & 4
%     \\
%   \hline
%  \end{tabular}
% \end{center}
% \vspace*{-0.05in}
%  \caption{Average number of points and strokes for each sketch category rounded to closest integer. \textit{Object/Scenes} shows the highest number of points and second highest number of strokes which implies that this category could be reduced when limiting the size of a sketch.}
%  \label{tab:study1_sketch_lengths}
% \end{table*}

% % MAYBE THIS SECTION ISN'T THAT IMPORTANT
% % Participant observation showed that some participants overshot the canvas border especially when sketching quickly. In addition, the interface did not allow to draw dots which deterred some participants.

% \subsection{Iterative Interface Development}\label{sec:iterative_interface_design}

% \begin{table*}[h!]
%  \begin{center}
%  \resizebox{\textwidth}{!}{
%  \begin{tabular}{p{0.73\linewidth}|p{0.33\linewidth}|p{0.23\linewidth}|}
%     % Iterative design process 
%     \cline{2-3}
%     \multirow{4}{\linewidth}{
%     \vspace{0cm}
%     \includegraphics[width=1\linewidth]{Images/Interface_1.png}
%     }
%     & 
%     \textbf{Evaluation}
%     &
%     \textbf{Participants}
%     \\
%     \cline{2-3}
    
%     &
%     Within-subject A/B testing. Simplification is applied after a stroke is completed for option A and while sketching for option B. Evaluation with System Usability Scale (SUS) and feedback form.
    
%     &
%     13 (6 female, 5 male, 2 other), mean age 33.46 (\sigma = 10.02)
   
%     \\
%     \cline{2-3}
    
%     &
%     \textbf{Usability}
%     & 
%     \textbf{Abstract}
%     \\
%   \cline{2-3}
    
%     &
%     5 median for thinking was easy to use (1-completely disagree to 5-completely agree)
%     &
%     68{\%} (35/52 sketches)
%     \\
%   \cline{2-3}
 
% \end{tabular}}
% \end{center}
% \vspace*{-0.05in}
%  \caption{First design iteration: responsive canvas size, unlimited points and stroke simplification, options to undo last stroke and reset canvas. To allow for smoother sketching, the stroke width was increased from 2 to 6 pixels, stroke simplification with the Ramer-Douglas-Peucker algorithm (RDP) using an epsilon value of 2 was introduced and stroke points were connected with Catmull-Rom splines rather than straight lines using the \textit{createVertex()} function in \textit{p5.js}. }
%  \label{tab:interface_iteration1_specs}
% \end{table*}

% \begin{table*}[h!]
%  \begin{center}
%  \resizebox{\textwidth}{!}{
%  \begin{tabular}{p{0.73\linewidth}|p{0.33\linewidth}|p{0.23\linewidth}|}
%     % Iterative design process 
%     \cline{2-3}
%     \multirow{4}{\linewidth}{
%     \vspace{0cm}
%     \includegraphics[width=1\linewidth]{Images/Interface_2.png}}
%     & 
%     \textbf{Evaluation}
%     &
%     \textbf{Participants}
%     \\
%     \cline{2-3}
    
%     &
%     Evaluation through simplified, customised SUS and feedback form. 
    
%     &
%     14 (5 female, 8 male, 1 prefer not to say), mean age 34.29 (\sigma = 9.44)
   
%     \\
%     \cline{2-3}
    
%     &
%     \textbf{Usability}
%     & 
%     \textbf{Abstract}
%     \\
%   \cline{2-3}
    
%     &
%     2.5 median for thinking the interface allowed to be expressive, 5 median for thinking was easy to use (1-completely disagree to 5-completely agree)
%     &
%     100{\%} (70/70 sketches) 
%     \\
%     \cline{2-3}
 
% \end{tabular}}
% \end{center}
% \vspace*{-0.05in}
%  \caption{Second interface iteration. Responsive canvas size, limited to one stroke and 150 points with stroke simplification, no undo or reset options. }
%  \label{tab:interface_iteration2_specs}
% \end{table*}

% \begin{table*}[h!]
%  \begin{center}
%  \resizebox{\textwidth}{!}{
%  \begin{tabular}{p{0.73\linewidth}|p{0.33\linewidth}|p{0.23\linewidth}|}
%     \cline{2-3}
%     \multirow{4}{\linewidth}{
%     \vspace{0cm}
%     \includegraphics[width=1\linewidth]{Images/Interface_3.png}
%     }
%     & 
%     \textbf{Evaluation}
%     &
%     \textbf{Participants}
%     \\
%     \cline{2-3}
    
%     &
%     Evaluation through simplified, customised SUS and feedback form. 
    
%     &
%     20 (12 female, 5 male, 2 other, 1 prefer not to say), mean age 31.85 (\sigma = 6.25)
   
%     \\
%     \cline{2-3}
    
%     &
%     \textbf{Usability}
%     & 
%     \textbf{Abstract}
%     \\
%   \cline{2-3}
    
%     &
%     4 median for thinking the interface allowed to be expressive, 4 median for thinking was easy to use (1-completely disagree to 5-completely agree)
%     &
%     96{\%} (96/100 sketches) 
%     \\
%     \cline{2-3}
 
% \end{tabular}}
% \end{center}
% \vspace*{-0.05in}
%  \caption{Third interface iteration. Responsive canvas size, limited to 150 points with unlimited strokes, stroke simplification, reset option and simple sketch length indicator.  }
%  \label{tab:interface_iteration3_specs}
% \end{table*}

% The interface was developed further in three successive usability studies. As described in Section~\ref{sec:sketch_categorisation} the analysis of Study Two follows the \textit{SketchRNN} architecture based on the \textit{Quick, Draw!} dataset. Therefore, elements of the \textit{Quick, Draw!} interface were incorporated in the development. This included a responsive canvas with a white background adapting to the browser window size and, to provide a smoother sketching experience the stroke width was increased and stroke simplification and a different line-drawing algorithm were employed as described in Table~\ref{tab:interface_overview} All following studies were conducted online to reach a more diverse participant pool and allow people to use devices that they are familiar with. All three design studies used synthesiser sounds from the NSynth dataset~\cite{engelNeuralAudioSynthesis2017} and a study about timbral differences for synthesis~\cite{vahidi2021modulation}.
% The first iteration was evaluated through A/B testing. For option A, simplification was applied after stroke completion and for Interface B it was applied in real-time while sketching. The design was chosen to test if real-time simplification will make it more difficult to sketch detail and thus encourage abstract sketches. For each interface option participants were first given two trial rounds, prompting them to sketch a cat and a calm sound for option A and a dog and noisy sound for option B before being asked to sketch their representation of two sound stimuli. Participants were given the option to undo the last stroke or reset the canvas. Feedback for each option was collected with the system usability scale (SUS) and option for free-form written responses. In addition, participants were asked to give their preferred interface after completing both options. Qualitative analysis of cat and dog sketches did not show that real-time simplification had any effect on the detail of sketches. Asking participants to sketch in a figurative way might actually incentivize them to maintain that approach throughout the study which might explain the low percentage of abstract sketches compared to the other studies. Table~\ref{tab:SUS results} shows that no significant differences in participant evaluation of the two interfaces on the SUS could be found with both options receiving positive ratings. Most participants preferred option A even though some did not seem to perceive a difference between them stating that 'both interfaces were the same' (P4) or that they '[â€¦] didn't notice they were different' (P10). The wording of the SUS appeared to be confusing to some participants who asked 'What is the system?' (P11) or 'I'm not sure what I am giving feedback on - there was little ''interface`` involved' (P6). Clickstream analysis showed that on average participants used the undo function six times per task. Based on this feedback, the following interfaces only applied stroke simplification after completion. For evaluation, the SUS was reduced to five modified questions seen in Figure~\ref{fig:iteration2_3_survey} to measure the interface's expressiveness and ease to use. As simplification alone did not seem to have an impact on representational style, the second interface iteration adopted a restrictive setup allowing only one stroke with a maximum of 150 points. Starting a new stroke automatically erased the previous one removing the need for a reset or undo option. While this setup produced exclusively abstract sketch representations, participants criticized its restrictiveness saying that 'It was very challenging expressing ideas with a single stroke' P(3) or 'I couldn't articulate my thoughts about each sound with only one very limited stroke' P(1). This sentiment is also reflected in the quantitative survey responses presented in Figure~\ref{fig:iteration2_survey}.


% The third iteration allowed an unlimited number of strokes while keeping the 150 point limit. Rather than stopping when reaching the limit, points were erased from the beginning of the sketch. An icon in the top left corner of the canvas indicated if the limit was reached by turning from an outlined green circle to a filled one. A red \textit{forbidden} icon indicated that a participant did not reach the minimum threshold that was introduced to prevent short or empty sketch submissions. A reset option was provided to clear the canvas. On average, participants gave high ratings for expressiveness while almost exclusively producing abstract representations. However, some participants were put off by the `erased' strokes when the limit was reached stating that `I didn't like that it deleted my drawing without warning' P(3) and `I was very conscious of the line eventually retracting, and I think I started to think more about ''how much space do I have left'' rather than the sound itself' P(1).
 
% \subsection{Interface Study Two}
% \label{sec:study2_interface_design}

% \begin{table*}[h!]
%  \begin{center}
%  \resizebox{\textwidth}{!}{
%  \begin{tabular}{p{0.73\linewidth}|p{0.33\linewidth}|p{0.23\linewidth}|}
% %   Study Two

%     \cline{2-3}
%     \multirow{4}{\linewidth}{
%         \vspace{0cm}
%     \includegraphics[width=1\linewidth]{Images/Interface_Study2.png}
%     }
%     & 
%     \textbf{Evaluation}
%     &
%     \textbf{Participants}
%     \\
%     \cline{2-3}
    
%     &
%     Interface used in Study Two. Evaluation through simplified, customised SUS and feedback form. 
%     &
%     88 participants (48 female, 38 male, 2 other), mean age 22.55 (\sigma = 4.56)
   
%     \\
%     \cline{2-3}
    
%     &
%     \textbf{Usability}
%     & 
%     \textbf{Abstract}
%     \\
%   \cline{2-3}
    
%     &
%     4 median for thinking the interface allowed to be expressive, 5 median for thinking was easy to use (1-completely disagree to 5-completely agree)
%     &
%     99{\%} (1742/1760 sketches)
%     \\
%   \cline{2-3}
 
% \end{tabular}}
% \end{center}
% \vspace*{-0.05in}
%  \caption{Interface for Study Two. Responsive canvas size, limited to 150 points with unlimited strokes, stroke simplification, reset option and improved sketch length indicator. }
%  \label{tab:interface_study2_specs}
% \end{table*}


% The last iteration of the interface design study provided a good balance between expressiveness and guidance towards abstract representation. The same setup was used for Study Two with an improved stroke length indicator that displayed the current length as a filling-up progress bar with markers for the lower and upper point limit. As the study's focus lied on collecting a sound-sketch dataset rather than testing usability, only two questions were asked about participants' experiences with the interface as seen in Figure~\ref{fig:interface_study2_survey}. The results confirm the positive responses of the interface study's last iteration while also almost exclusively producing abstract representations. A reoccurring feedback for all interfaces was a request for more visual features like colour, varying stroke width or texture and different input modes like a stylus pen or touchscreen. While investigating other visual dimensions is beyond the scope of this research, extending the interaction to touchscreen on laptops, smartphones and tablets might improve the expressiveness for participants potentially enabling them to translate their associations more precisely with an otherwise unchanged interface.   


\section{Discussion}\label{sec:discussion}


\subsection{Abstract and figurative representations of sound stimuli}\label{subsec:disc_sketch_categorisation}
The sound-sketches collected in this research were categorised through a rigorous, human-centred process consisting of participant interviews and a card-sorting study. On the highest level, sound-sketches were divided into two groups: \textit{abstract} and \textit{figurative}. Abstract representations appear to be strongly informed by cross-modal associations and show many similarities to visual stimuli used in matching tasks. Figurative representations, on the other hand, depict objects or scenes associated with a sound that might show the sound source like a musical instrument directly or could be informed by an emotional response or specific memory, for example a scary sound might be represented with a scary scene informed by the memory of a movie scene similar to the sketch in Figure~\ref{fig:scary_scene}. Which representational approach a participant took was influenced by the type of sound or a participant's experience. Figurative representations were found to be more prevalent among non-musicians and for sounds with an easily identifiable sound source like a musical instrument. This could be interpreted as a difference in listening modes: coined by \citeA{chion}, the \textit{reduced listening} mode describes a focus on the sound itself as opposed to \textit{semantic listening} which focuses on the source or meaning of a sound. Described as hardly natural by \citeA{chion}, \textit{reduced listening} can be more demanding for the untrained ear, especially for sounds from a familiar source. This analysis suggests that two different approaches would be needed when computationally mapping sketches to sound. For abstract sketches that encode information about sound characteristics directly in their form, extracting quantitative sketch features like the ones described in Section~\ref{subsec:feature_analysis} appears to be a viable approach. For figurative sketches, a more suitable approach might include object recognition for sketches of musical instruments and other sound-producing objects or sentiment analysis for emotionally informed scenes. 
\\ \\
Section~\ref{subsec:feature-analysis} shows that for Study One multiple significant correlations were found between audio and sketch features despite including abstract and figurative sketches. With 84{\%} of sketches classified as abstract, figurative sketches did not appear to impact these correlations. Because of the small sample size differences in correlations between abstract and figurative sketches could not be investigated in a meaningful way. Figurative representations might still be indirectly influenced by cross-modal associations, for example, an uncomfortable noisy and dissonant sound might be represented with a scene similar to the design \textit{Landscape of Thorns} by \citeA{Michael_Brill_Safdar_Abidi} that aims to communicate discomfort and danger through sharp and jagged structures. On the other hand, abstract representations might include references to symbolic representations of sound that are not based on cross-modal associations. The feedback analysis in Section~\ref{sec:interview_analysis} reveals that some sketches in the \textit{Lines} category in particular are informed by audio representations like waveforms, spectrograms or amplitude envelopes. 
\\ \\
As described in Section~\ref{subsec:sound-shape}, cross-modal associations are not fixed; they emerge in different forms depending on the stimulus and situation and sketch representations might incorporate multiple associations or references to objects, scenes or symbols. Without explicitly asking a participant what they had in mind, it may not be possible to exactly determine their underlying motivation. Generally, only a few participants strictly adhered to one representational approach. While dominated by a primary approach, most participants switched between or mixed representational styles throughout the sketching task.

\subsection{Cross-modal associations in free-form sketches}
Compared to the figurative category \textit{Object/Scenes} the abstract categories \textit{Chaotic/Jagged},\textit{Grains},\textit{Lines} and \textit{Radiating/Rounds} appear to be primarily influenced by cross-modal associations. This research aimed to investigate sound-shape associations specifically, but it might be useful to define the terminology to better interpret the results. The visual stimuli used in sound-shape matching tasks like \citeA{adeliAudiovisualCorrespondenceMusical2014} focus very strictly on shape meaning that stimuli only show the outlining \textit{form} through a non-intersecting, connected series of lines. Looking at Figures~\ref{fig:study_one_categories}~and~\ref{fig:study_two_categories} it is obvious that participants did not only use \textit{form} in their abstract sketches. \citeA{kneesSearchingAudioSketching2016} made similar findings and in their sound-sketch prototype included tools to create outlines of simple forms like circles and triangles that can be given more \textit{complexity} through free-form lines and filled with \textit{textures}. In this research, the abstract categories \textit{Chaotic/Jagged} and \textit{Radiating/Round} appear to encode sound characteristics through their \textit{form} with the former found more frequently for warm sounds in Study One and the latter for rough sounds in both studies which aligns with existing sound-shape research~\cite{kohler1929gestalt,adeliAudiovisualCorrespondenceMusical2014}. However, \textit{Chaotic/Jagged} appears to be also contrasted by \textit{Lines} which is more frequently found in the least rough sounds. Sketches in the \textit{Lines} category have considerably fewer sharp angles than sketches in \textit{Chaotic/Jagged} which could be interpreted as a difference in \textit{form}, but \textit{Lines} also exhibits considerably fewer stroke intersections than \textit{Chaotic/Jagged} which can be interpreted as a difference in \textit{complexity}. The remaining abstract category \textit{Grains} was particularly prevalent for the intersected sounds \textit{Crackles} and \textit{String Grains} in Study One and is described to consist of small, repetitive components which can be interpreted as \textit{texture} and was quantified by the average number of strokes and the average stroke length. Strictly speaking, sound-shape associations are not the only cross-modal associations that influence abstract representations, however, in this context the definition of \textit{shape} could be extended to describe any 2-dimensional structure composed of straight or curved lines. For quantitative analysis, this broader definition of shape might be sufficient as the sketch features described in Section~\ref{subsec:feature_analysis} capture more than just the \textit{form} of a sketch. 
\\ \\
The results of the feature correlation analysis in Section~\ref{subsec:feature_correlation} support that typical sound-shape associations influenced sketch representations with acute angles positively correlating with attributes like \textit{Sharpness}, \textit{Roughness} and \textit{Brightness} and negatively with \textit{Warmth} and \textit{Boominess}. The sketch feature \textit{Wide Curves} which was expected to quantify roundness did not show any significant correlations in Study One, but multiple significant correlations for Study Two that point in the opposite direction of \textit{acute angles}. This might be due to noise introduced by higher variance in sketch approaches in Study One, but could suggest that the extracted features are not a good measure for the overall roundness of a sketch. \textit{Intersections} proved to be meaningful in describing cross-modal associations showing multiple significant correlations with audio features across both studies. The direction of these correlations is the same for acute angles, leading to the belief that the opposing pairs \textit{rough} and \textit{soft} for sound might not only be visualised through jaggedness or roundness but also through complexity and simplicity. Overall, the \textit{roughness} of a sound appears to have a strong influence on how sketches are represented which is clearly visualised in Figure~\ref{fig:study2_categories_by_sound}. The figure also suggests that the \textit{Lines} category is more prevalent for \textit{thin} sounds, but the differences found in this study did not prove to be significant. As the sketch and audio features used do not directly quantify thinness, feature correlation could not provide any additional information. Further work could focus on this attribute of sound which would provide an additional timbral dimension encoded in sound-sketch representations. 
\\ \\ 
A general problem that emerged in both studies is that it cannot be clearly determined which sound characteristic participants focused on. As a participant in Study One stated, there are a lot of different aspects of sound and it is difficult to represent them all in a simple sketch. This might be made even more difficult with the sketch length limit that was introduced in Study Two. While it succeeded in guiding participants to represent sounds in more abstract ways, it does prevent them from elaborating or overlaying multiple approaches that might capture more aspects of a sound. It might be possible that thinness or thickness can be encoded through sketching, but participants mainly focused on roughness when representing sound. For future research, it could be considered to also ask participants to rate which attributes of the sound they focused on with a design similar to \citeA{hayes2021perceptual}. Another approach could ask participants to imagine a sound with a certain attribute, rather than presenting them with an actual sound stimulus. This was done in the test session of Study Two where participants were asked to draw a noisy and calm sound. Through this, it was made clear which attribute the participant had in mind while sketching. 

\subsection{Implications for development of a sketch-based sound synthesizer}\label{subsec:disc_sketch-based_synth}

Study Two was designed with a future implementation of a sketch-based sound synthesizer in mind. The results can shed light on which mapping architecture is more appropriate for such a system:
\begin{itemize}
    \item a regression approach where a change in timbre is induced by an incremental change in the corresponding sketch feature. For example a sound would become noisier with an increase sharp angles. 
    \item a classification approach where the overall timbre category is determined. In this scenario, a sketch could represent multiple categories for example either \textit{rough} or \textit{soft} in combination with either \textit{thick} or \textit{thin}.
\end{itemize}
   
The feature correlations in Section~\ref{subsec:feature_correlation} describe a monotonic relationship between audio and sketch features which can lead to the conclusion that a gradual increase in an audio feature like \textit{roughness} coincides with a gradual increase in a sketch feature like \textit{acute angles}. However, it has to be remembered that these sketch features represent the average sketch of the participants meaning that a single participant might not follow a change of timbre in such an incremental fashion rather participants could think of sounds in categories and with rising \textit{roughness} an increasing number of participants switch from round to jagged shapes. Similarly, statistical agreement for sketch representations was only determined for an average participant in Section~\ref{subsec:inter-rater_reliability} and, in this research, significant results could not be obtained when looking at individual participants. In addition, if a specific, computed audio feature increases linearly for a series of sounds, it does not mean that humans would perceive this as a linear change in timbre. When thinking about a sound that is neither particularly \textit{rough} nor \textit{soft}, it is likely that another more prominent feature, for example, \textit{thick} or \textit{thin} would inform the perception of this sound and hence the sketch representation of it. In fact, \citeA{ben's_newest_paper_about_the_timbre_explorer_study} found that relatively small, localised clusters emerge when participants were asked to define sound within a timbre space according descriptors like \textit{rough} or \textit{soft}. A linear interpolation between the parameters of two FM synthesizer sounds, might not be perceived as a linear transition in sound characteristics, but rather different sound environments that emerge along this path. Given this analysis, a categorisation approach appears to be more viable when designing a sketch-based sound synthesiser. It should also be added that sketch to synthesizer mappings do not have to be limited to timbre. An advantage of this design is that multiple parameters could be represented with a single input. Revisiting the research of~\citeA{spence_paper} and~\citeA{kussnerMusiciansAreMore2014} shows that sketch representations can also serve to represent pitch or amplitude envelopes. Future work can explore how a sketch input could manipulate these parameters at the same time, potentially using a combination of mappings that are established through a data-driven machine learning approach and hard-coded mappings that are based on findings from empirical studies. 

\section{Conclusion}\label{sec:conclusion}
In two participant studies, 2320 free-form sound-sketches (560 from Study One and 1760 from Study Two) were collected with simple, monochromatic digital sketching interfaces. Through a rigorous human-centred analysis sketches were categorised by their primary representational approach. Several significant correlations between quantitative audio and sketch features were found that align with findings from cross-modal matching tasks. The results show that while sound-shape associations play a significant role in sketched representations, humans incorporate other visual aspects like structural complexity or texture as well or chose figurative representations of emotions or sound-producing objects. Some level of agreement on how to represent sounds could be found between participants which appears strongest for sounds that are dominated by one sound characteristic. This research provides useful insights and suggestions for designing a sketch-based sound synthesiser.

\bibliographystyle{apacite}
\bibliography{interactapasample}

% \section{Appendix}

% \begin{table*}[h!]
%  \begin{center}
%  \begin{tabular}{|p{0.35\linewidth}|p{0.025\linewidth}|p{0.025\linewidth}||p{0.35\linewidth}|p{0.025\linewidth}|p{0.025\linewidth}|}
%  \hline
%  \multicolumn{1}{c|}{}
%     & \textbf{A} & \textbf{B} & 
%     \multicolumn{1}{c|}{} &
%     \textbf{A} & \textbf{B} \\
%   \hline
%   I think I would like to use this system frequently.
%   & 4 & 3 & 
%   I thought there was too much inconsistency in this system.
%   & 1 & 1 \\
%   \hline
%   I found the system unnecessarily complex.
%   & 2 & 1 &  
%   I would imagine that most people would learn to use this system very quickly.
%   & 4 & 4 \\
%   \hline
%   I thought the system was easy to use.
%   & 2 & 1 &  
%   I found the system very cumbersome to use.
%   & 1 & 1 \\
%   \hline
%     I think that I would need the support of a technical person to be able to use this system.
%   & 2 & 1 &  
%   I felt very confident using the system.
%   & 4 & 4 \\
%   \hline
%     I found the various functions in this system were well integrated.
%   & 2 & 1 &  
%   I needed to learn a lot of things before I could get going with the system.
%   & 4 & 4 \\
%   \hline
 
%  \end{tabular}
% \end{center}
% \vspace*{-0.05in}
%  \caption{Median scores on the System Usability Scale (SUS) for A (post-stroke simplification) and B (real-time simplification) options of first interface iteration (Likert Scale from 1-strongly disagree to 5-strongly agree). No significant difference between options could be found with Mann-Whitney U-test (p$>$0.39 for all), but option A was preferred by ten from thirteen participants}
%  \label{tab:SUS results}
% \end{table*}

% \begin{figure}[!ht]
%     \subfloat[Iteration Two\label{fig:iteration2_survey}]{%
%       \includegraphics[width=0.5\textwidth]{Images/interfac2_sus.png}
%     }
%     \hfill
%     \subfloat[Iteration Three\label{fig:iteration3_survey}]{%
%       \includegraphics[width=0.5\textwidth]{Images/interfac3_sus.png}
%     }
%     \caption{Survey responses from  for iteration two and three of the iterative interface design. Likert scale from 1 (completely disagree) to 5 (completely agree).} 
%     \label{fig:iteration2_3_survey}
%   \end{figure}
  
% \begin{figure}[h]
% \centering
% \includegraphics[width=0.5\columnwidth]{Images/Interface_study2_sus.png}
% \caption{Survey responses for usability of Study Two interface \label{fig:interface_study2_survey}}
% \end{figure}


% \begin{table*}[h!]
%  \begin{center}
%  \resizebox{\textwidth}{!}{
%  \begin{tabular}{|p{0.7\linewidth}|p{0.33\linewidth}|p{0.23\linewidth}|}
% %  Study One
%   \cline{1-1}
%     \textbf{Interface Study One}
%     &
%     \multicolumn{2}{|l}{}
%     \\
%     \hline
%     \multirow{4}{\linewidth}{
%     \vspace{0cm}
%     \includegraphics[width=0.5\linewidth]{Images/Interface_Study1.png}
%     Fixed 750x750 pixel sketching canvas, unlimited points and strokes without simplification, no undo or reset options
%     }
%     & 
%     \textbf{Evaluation}
%     &
%     \textbf{Participants}
%     \\
%     \cline{2-3}
    
%     &
%     Interface used in Study One (Section~\ref{sec:study1} and evaluated through participant observation and interviews.
%     &
%     28 (14 female, 14 male), median age category 26-33
   
%     \\
%     \cline{2-3}
    
%     &
%     \textbf{Usability}
%     & 
%     \textbf{Abstract}
%     \\
%   \cline{2-3}
    
%     &
%     1 median interface satisfaction (1-very unsatisfied to 5-very satisfied) quantified from interviews 
%     &
%     85{\%} (474/560 sketches)
%     \\
%     [5.0ex]
%   \hline
%     \cline{1-1}
%     % Iterative design process 
%     \textbf{Iterative Design Process}
%     &
%     \multicolumn{2}{|l}{}
%     \\
%     \hline
%     \multirow{4}{\linewidth}{
%     \vspace{0cm}
%     \includegraphics[width=0.75\linewidth]{Images/Interface_1.png}
%     Responsive canvas size, unlimited points and stroke simplification, options to undo last stroke and reset canvas
%     }
%     & 
%     \textbf{Evaluation}
%     &
%     \textbf{Participants}
%     \\
%     \cline{2-3}
    
%     &
%     Within-subject A/B testing. Simplification is applied after a stroke is completed for option A and while sketching for option B. Evaluation with System Usability Scale (SUS) and feedback form.
    
%     &
%     13 (6 female, 5 male, 2 other), mean age 33.46 (\sigma = 10.02)
   
%     \\
%     \cline{2-3}
    
%     &
%     \textbf{Usability}
%     & 
%     \textbf{Abstract}
%     \\
%   \cline{2-3}
    
%     &
%     5 median for thinking was easy to use (1-completely disagree to 5-completely agree)
%     &
%     68{\%} (35/52 sketches)
%     \\
%     [5.0ex]
%   \hline
%       \hline
%     \multirow{4}{\linewidth}{
%     \vspace{0cm}
%     \includegraphics[width=0.75\linewidth]{Images/Interface_2.png}
        
%     Responsive canvas size, limited to one stroke and 150 points with stroke simplification, no undo or reset options.
    

%     }
%     & 
%     \textbf{Evaluation}
%     &
%     \textbf{Participants}
%     \\
%     \cline{2-3}
    
%     &
%     Evaluation through simplified, customised SUS and feedback form. 
    
%     &
%     14 (5 female, 8 male, 1 prefer not to say), mean age 34.29 (\sigma = 9.44)
   
%     \\
%     \cline{2-3}
    
%     &
%     \textbf{Usability}
%     & 
%     \textbf{Abstract}
%     \\
%   \cline{2-3}
    
%     &
%     2.5 median for thinking the interface allowed to be expressive, 5 median for thinking was easy to use (1-completely disagree to 5-completely agree)
%     &
%     100{\%} (70/70 sketches) 
%     \\
%     [5.0ex]
%     \hline
%         \hline
%     \multirow{4}{\linewidth}{
%     \vspace{0cm}
%     \includegraphics[width=0.75\linewidth]{Images/Interface_3.png}
%     Responsive canvas size, limited to 150 points with unlimited strokes, stroke simplification, reset option and simple sketch length indicator. 
%     }
%     & 
%     \textbf{Evaluation}
%     &
%     \textbf{Participants}
%     \\
%     \cline{2-3}
    
%     &
%     Evaluation through simplified, customised SUS and feedback form. 
    
%     &
%     20 (12 female, 5 male, 2 other, 1 prefer not to say), mean age 31.85 (\sigma = 6.25)
   
%     \\
%     \cline{2-3}
    
%     &
%     \textbf{Usability}
%     & 
%     \textbf{Abstract}
%     \\
%   \cline{2-3}
    
%     &
%     4 median for thinking the interface allowed to be expressive, 4 median for thinking was easy to use (1-completely disagree to 5-completely agree)
%     &
%     96{\%} (96/100 sketches) 
%     \\
%     [5.0ex]
%     \hline
%     \hline
% %   Study Two
%     \cline{1-1}
%     \textbf{Interface Study Two}
%     &
%     \multicolumn{2}{|l}{}
%     \\
%     \hline
%     \multirow{4}{\linewidth}{
%         \vspace{0cm}
%     \includegraphics[width=0.75\linewidth]{Images/Interface_Study2.png}
%     Responsive canvas size, limited to 150 points with unlimited strokes, stroke simplification, reset option and improved sketch length indicator.
%     }
%     & 
%     \textbf{Evaluation}
%     &
%     \textbf{Participants}
%     \\
%     \cline{2-3}
    
%     &
%     Interface used in Study Two. Evaluation through simplified, customised SUS and feedback form. 
%     &
%     88 participants (48 female, 38 male, 2 other), mean age 22.55 (\sigma = 4.56)
   
%     \\
%     \cline{2-3}
    
%     &
%     \textbf{Usability}
%     & 
%     \textbf{Abstract}
%     \\
%   \cline{2-3}
    
%     &
%     4 median for thinking the interface allowed to be expressive, 5 median for thinking was easy to use (1-completely disagree to 5-completely agree)
%     &
%     99{\%} (1742/1760 sketches)
%     \\
%     [10.0ex]
%   \hline
 
% \end{tabular}}
% \end{center}
% \vspace*{-0.05in}
%  \caption{Overview of the interface design stages. The first interface featured white strokes on a black background which was reversed for all following interfaces. To allow for smoother sketching, the stroke width was increased from 2 to 6 pixels, stroke simplification with the Ramer-Douglas-Peucker algorithm (RDP) using an epsilon value of 2 was introduced and stroke points were connected with Catmull-Rom splines rather than straight lines using the \textit{createVertex()} function in \textit{p5.js}. }
%  \label{tab:interface_overview}


\end{document}
